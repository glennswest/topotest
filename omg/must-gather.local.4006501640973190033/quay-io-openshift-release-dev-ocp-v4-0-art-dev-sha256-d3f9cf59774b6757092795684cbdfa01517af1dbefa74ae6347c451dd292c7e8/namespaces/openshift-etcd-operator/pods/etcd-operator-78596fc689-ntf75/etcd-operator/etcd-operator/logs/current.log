2023-05-04T16:31:28.440867277Z I0504 16:31:28.440775       1 observer_polling.go:52] Starting from specified content for file "/var/run/secrets/serving-cert/tls.crt"
2023-05-04T16:31:28.441027288Z I0504 16:31:28.441000       1 observer_polling.go:52] Starting from specified content for file "/var/run/secrets/serving-cert/tls.key"
2023-05-04T16:31:28.441074416Z I0504 16:31:28.440847       1 profiler.go:21] Starting profiling endpoint at http://127.0.0.1:6060/debug/pprof/
2023-05-04T16:31:28.441296413Z I0504 16:31:28.441282       1 observer_polling.go:159] Starting file observer
2023-05-04T16:31:28.441363418Z I0504 16:31:28.441354       1 cmd.go:209] Using service-serving-cert provided certificates
2023-05-04T16:31:28.441395549Z I0504 16:31:28.441383       1 observer_polling.go:135] File observer successfully synced
2023-05-04T16:31:28.441436946Z I0504 16:31:28.441428       1 observer_polling.go:74] Adding reactor for file "/var/run/secrets/serving-cert/tls.crt"
2023-05-04T16:31:28.441504283Z I0504 16:31:28.441495       1 observer_polling.go:74] Adding reactor for file "/var/run/secrets/serving-cert/tls.key"
2023-05-04T16:31:28.441552203Z I0504 16:31:28.441543       1 observer_polling.go:52] Starting from specified content for file "/var/run/configmaps/config/config.yaml"
2023-05-04T16:31:28.441830555Z I0504 16:31:28.441813       1 observer_polling.go:159] Starting file observer
2023-05-04T16:31:28.442193977Z I0504 16:31:28.442131       1 observer_polling.go:135] File observer successfully synced
2023-05-04T16:31:28.456122080Z I0504 16:31:28.456090       1 builder.go:262] openshift-cluster-etcd-operator version 4.12.0-202301311142.p0.ga9aaf7d.assembly.stream-a9aaf7d-a9aaf7d163b9f118347aca4080ce8cab746b241e
2023-05-04T16:31:28.456474361Z I0504 16:31:28.456459       1 dynamic_serving_content.go:113] "Loaded a new cert/key pair" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2023-05-04T16:31:28.667027435Z I0504 16:31:28.666992       1 requestheader_controller.go:244] Loaded a new request header values for RequestHeaderAuthRequestController
2023-05-04T16:31:28.669996556Z I0504 16:31:28.669777       1 maxinflight.go:140] "Initialized nonMutatingChan" len=400
2023-05-04T16:31:28.669996556Z I0504 16:31:28.669791       1 maxinflight.go:146] "Initialized mutatingChan" len=200
2023-05-04T16:31:28.669996556Z I0504 16:31:28.669807       1 timing_ratio_histogram.go:202] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=[executing readOnly]
2023-05-04T16:31:28.669996556Z I0504 16:31:28.669816       1 timing_ratio_histogram.go:202] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=[executing mutating]
2023-05-04T16:31:28.669996556Z I0504 16:31:28.669823       1 maxinflight.go:117] "Set denominator for readonly requests" limit=400
2023-05-04T16:31:28.669996556Z I0504 16:31:28.669828       1 maxinflight.go:121] "Set denominator for mutating requests" limit=200
2023-05-04T16:31:28.669996556Z I0504 16:31:28.669844       1 config.go:731] Not requested to run hook priority-and-fairness-config-consumer
2023-05-04T16:31:28.675033968Z W0504 16:31:28.674999       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2023-05-04T16:31:28.675033968Z W0504 16:31:28.675016       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2023-05-04T16:31:28.675248641Z I0504 16:31:28.675234       1 genericapiserver.go:480] MuxAndDiscoveryComplete has all endpoints registered and discovery information is complete
2023-05-04T16:31:28.679686197Z I0504 16:31:28.679589       1 builder.go:403] detected SingleReplicaTopologyMode, the original leader election has been altered for the default SingleReplicaToplogy
2023-05-04T16:31:28.679741671Z I0504 16:31:28.679730       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-etcd-operator.svc\" [serving] validServingFor=[metrics.openshift-etcd-operator.svc,metrics.openshift-etcd-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1683030520\" (2023-05-02 12:31:25 +0000 UTC to 2025-05-01 12:31:26 +0000 UTC (now=2023-05-04 16:31:28.679582693 +0000 UTC))"
2023-05-04T16:31:28.679854853Z I0504 16:31:28.679844       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1683217888\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1683217888\" (2023-05-04 15:31:28 +0000 UTC to 2024-05-03 15:31:28 +0000 UTC (now=2023-05-04 16:31:28.679824647 +0000 UTC))"
2023-05-04T16:31:28.679887504Z I0504 16:31:28.679879       1 secure_serving.go:210] Serving securely on [::]:8443
2023-05-04T16:31:28.679918803Z I0504 16:31:28.679900       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2023-05-04T16:31:28.679925927Z I0504 16:31:28.679921       1 shared_informer.go:255] Waiting for caches to sync for RequestHeaderAuthRequestController
2023-05-04T16:31:28.679950262Z I0504 16:31:28.679942       1 genericapiserver.go:585] [graceful-termination] waiting for shutdown to be initiated
2023-05-04T16:31:28.679982022Z I0504 16:31:28.679944       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2023-05-04T16:31:28.680044760Z I0504 16:31:28.680034       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2023-05-04T16:31:28.680066250Z I0504 16:31:28.680053       1 reflector.go:221] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:31:28.680066250Z I0504 16:31:28.680062       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:31:28.680089654Z I0504 16:31:28.679955       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2023-05-04T16:31:28.680095966Z I0504 16:31:28.680089       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2023-05-04T16:31:28.680135710Z I0504 16:31:28.680124       1 reflector.go:221] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:31:28.680135710Z I0504 16:31:28.680131       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:31:28.680194601Z I0504 16:31:28.679880       1 leaderelection.go:248] attempting to acquire leader lease openshift-etcd-operator/openshift-cluster-etcd-operator-lock...
2023-05-04T16:31:28.680233544Z I0504 16:31:28.680220       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2023-05-04T16:31:28.684323578Z I0504 16:31:28.680124       1 reflector.go:221] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:31:28.684323578Z I0504 16:31:28.680299       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:31:28.684323578Z I0504 16:31:28.680541       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2023-05-04T16:31:28.687748284Z I0504 16:31:28.687712       1 leaderelection.go:352] lock is held by etcd-operator-78596fc689-ntf75_344e8fcc-742a-4ad7-bb2b-8929d70c6447 and has not yet expired
2023-05-04T16:31:28.687748284Z I0504 16:31:28.687725       1 leaderelection.go:253] failed to acquire lease openshift-etcd-operator/openshift-cluster-etcd-operator-lock
2023-05-04T16:31:28.780864389Z I0504 16:31:28.780831       1 shared_informer.go:285] caches populated
2023-05-04T16:31:28.780864389Z I0504 16:31:28.780847       1 shared_informer.go:262] Caches are synced for RequestHeaderAuthRequestController
2023-05-04T16:31:28.780919412Z I0504 16:31:28.780831       1 shared_informer.go:285] caches populated
2023-05-04T16:31:28.780940532Z I0504 16:31:28.780931       1 shared_informer.go:285] caches populated
2023-05-04T16:31:28.780940532Z I0504 16:31:28.780936       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2023-05-04T16:31:28.780956933Z I0504 16:31:28.780949       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2023-05-04T16:31:28.781132983Z I0504 16:31:28.781116       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_aggregator-client-signer@1683098704\" [] issuer=\"<self>\" (2023-05-03 07:25:03 +0000 UTC to 2023-06-02 07:25:04 +0000 UTC (now=2023-05-04 16:31:28.781094601 +0000 UTC))"
2023-05-04T16:31:28.781274409Z I0504 16:31:28.781265       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-etcd-operator.svc\" [serving] validServingFor=[metrics.openshift-etcd-operator.svc,metrics.openshift-etcd-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1683030520\" (2023-05-02 12:31:25 +0000 UTC to 2025-05-01 12:31:26 +0000 UTC (now=2023-05-04 16:31:28.781250313 +0000 UTC))"
2023-05-04T16:31:28.781402248Z I0504 16:31:28.781393       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1683217888\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1683217888\" (2023-05-04 15:31:28 +0000 UTC to 2024-05-03 15:31:28 +0000 UTC (now=2023-05-04 16:31:28.78136028 +0000 UTC))"
2023-05-04T16:31:28.781553122Z I0504 16:31:28.781544       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:54 +0000 UTC to 2033-04-29 12:12:54 +0000 UTC (now=2023-05-04 16:31:28.781522334 +0000 UTC))"
2023-05-04T16:31:28.781587526Z I0504 16:31:28.781580       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:57 +0000 UTC to 2024-05-01 12:12:57 +0000 UTC (now=2023-05-04 16:31:28.781568661 +0000 UTC))"
2023-05-04T16:31:28.781626740Z I0504 16:31:28.781620       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:57 +0000 UTC to 2024-05-01 12:12:57 +0000 UTC (now=2023-05-04 16:31:28.781608335 +0000 UTC))"
2023-05-04T16:31:28.781665352Z I0504 16:31:28.781658       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:55 +0000 UTC to 2033-04-29 12:12:55 +0000 UTC (now=2023-05-04 16:31:28.781645575 +0000 UTC))"
2023-05-04T16:31:28.781707782Z I0504 16:31:28.781700       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1683031055\" [] issuer=\"<self>\" (2023-05-02 12:37:35 +0000 UTC to 2024-05-01 12:37:36 +0000 UTC (now=2023-05-04 16:31:28.781686311 +0000 UTC))"
2023-05-04T16:31:28.781740623Z I0504 16:31:28.781734       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-controller-manager-operator_csr-signer-signer@1683098709\" [] issuer=\"<self>\" (2023-05-03 07:25:09 +0000 UTC to 2023-07-02 07:25:10 +0000 UTC (now=2023-05-04 16:31:28.781721347 +0000 UTC))"
2023-05-04T16:31:28.781780709Z I0504 16:31:28.781774       1 tlsconfig.go:178] "Loaded client CA" index=6 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1683098906\" [] issuer=\"openshift-kube-controller-manager-operator_csr-signer-signer@1683098709\" (2023-05-03 07:28:25 +0000 UTC to 2023-06-02 07:28:26 +0000 UTC (now=2023-05-04 16:31:28.781760922 +0000 UTC))"
2023-05-04T16:31:28.781834820Z I0504 16:31:28.781822       1 tlsconfig.go:178] "Loaded client CA" index=7 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_aggregator-client-signer@1683098704\" [] issuer=\"<self>\" (2023-05-03 07:25:03 +0000 UTC to 2023-06-02 07:25:04 +0000 UTC (now=2023-05-04 16:31:28.781809152 +0000 UTC))"
2023-05-04T16:31:28.781956148Z I0504 16:31:28.781947       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-etcd-operator.svc\" [serving] validServingFor=[metrics.openshift-etcd-operator.svc,metrics.openshift-etcd-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1683030520\" (2023-05-02 12:31:25 +0000 UTC to 2025-05-01 12:31:26 +0000 UTC (now=2023-05-04 16:31:28.781933375 +0000 UTC))"
2023-05-04T16:31:28.782069521Z I0504 16:31:28.782061       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1683217888\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1683217888\" (2023-05-04 15:31:28 +0000 UTC to 2024-05-03 15:31:28 +0000 UTC (now=2023-05-04 16:31:28.782039985 +0000 UTC))"
2023-05-04T16:33:10.380638340Z I0504 16:33:10.380601       1 httplog.go:131] "HTTP" verb="GET" URI="/metrics" latency="2.643981ms" userAgent="Prometheus/2.39.1" audit-ID="84ac628e-6457-4444-807a-59876dd7fdd7" srcIP="10.128.0.90:46682" resp=200
2023-05-04T16:33:15.076476194Z I0504 16:33:15.076102       1 leaderelection.go:352] lock is held by etcd-operator-78596fc689-ntf75_344e8fcc-742a-4ad7-bb2b-8929d70c6447 and has not yet expired
2023-05-04T16:33:15.076476194Z I0504 16:33:15.076125       1 leaderelection.go:253] failed to acquire lease openshift-etcd-operator/openshift-cluster-etcd-operator-lock
2023-05-04T16:33:40.364678980Z I0504 16:33:40.364533       1 httplog.go:131] "HTTP" verb="GET" URI="/metrics" latency="2.446175ms" userAgent="Prometheus/2.39.1" audit-ID="2615baf9-abd1-477d-b7ae-7ae83070ed04" srcIP="10.128.0.90:46682" resp=200
2023-05-04T16:34:10.366407436Z I0504 16:34:10.366368       1 httplog.go:131] "HTTP" verb="GET" URI="/metrics" latency="3.800193ms" userAgent="Prometheus/2.39.1" audit-ID="bae6d5bd-9c6a-4449-a082-4a7b85250eb1" srcIP="10.128.0.90:46682" resp=200
2023-05-04T16:34:40.372005486Z I0504 16:34:40.371304       1 httplog.go:131] "HTTP" verb="GET" URI="/metrics" latency="8.603478ms" userAgent="Prometheus/2.39.1" audit-ID="40245ff0-6ac7-4b95-9f17-76db66b5e76d" srcIP="10.128.0.90:46682" resp=200
2023-05-04T16:34:50.818860389Z I0504 16:34:50.818825       1 leaderelection.go:352] lock is held by etcd-operator-78596fc689-ntf75_344e8fcc-742a-4ad7-bb2b-8929d70c6447 and has not yet expired
2023-05-04T16:34:50.818860389Z I0504 16:34:50.818838       1 leaderelection.go:253] failed to acquire lease openshift-etcd-operator/openshift-cluster-etcd-operator-lock
2023-05-04T16:35:10.363355280Z I0504 16:35:10.363315       1 httplog.go:131] "HTTP" verb="GET" URI="/metrics" latency="1.789068ms" userAgent="Prometheus/2.39.1" audit-ID="28941383-c272-4809-ad80-b3d93008d86e" srcIP="10.128.0.90:46682" resp=200
2023-05-04T16:35:40.364568183Z I0504 16:35:40.364531       1 httplog.go:131] "HTTP" verb="GET" URI="/metrics" latency="2.189164ms" userAgent="Prometheus/2.39.1" audit-ID="a871c9b6-1bfc-4832-9fbc-26a0ae7e5bc4" srcIP="10.128.0.90:46682" resp=200
2023-05-04T16:36:05.392775821Z I0504 16:36:05.392745       1 leaderelection.go:258] successfully acquired lease openshift-etcd-operator/openshift-cluster-etcd-operator-lock
2023-05-04T16:36:05.393116699Z I0504 16:36:05.392854       1 event.go:285] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"openshift-etcd-operator", Name:"openshift-cluster-etcd-operator-lock", UID:"de75fdb7-f1fb-431e-a183-3dfbd165ad9e", APIVersion:"v1", ResourceVersion:"633154", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' etcd-operator-78596fc689-ntf75_b613c34a-104a-4f03-81dc-e65a3a91ab9a became leader
2023-05-04T16:36:05.394517639Z I0504 16:36:05.394461       1 event.go:285] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-etcd-operator", Name:"openshift-cluster-etcd-operator-lock", UID:"e0f110e3-b453-471f-81ce-cddf393d846c", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"633155", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' etcd-operator-78596fc689-ntf75_b613c34a-104a-4f03-81dc-e65a3a91ab9a became leader
2023-05-04T16:36:05.412319199Z I0504 16:36:05.405180       1 base_controller.go:67] Waiting for caches to sync for MissingStaticPodController
2023-05-04T16:36:05.412319199Z I0504 16:36:05.409766       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "UnsupportedConfigOverridesController" resync interval is set to 0s which might lead to client request throttling
2023-05-04T16:36:05.412319199Z I0504 16:36:05.409790       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "LoggingSyncer" resync interval is set to 0s which might lead to client request throttling
2023-05-04T16:36:05.412319199Z I0504 16:36:05.409801       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "GuardController" resync interval is set to 0s which might lead to client request throttling
2023-05-04T16:36:05.412319199Z I0504 16:36:05.409838       1 reflector.go:221] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.409844       1 reflector.go:257] Listing and watching *v1.Secret from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410048       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "UnsupportedConfigOverridesController" resync interval is set to 0s which might lead to client request throttling
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410090       1 reflector.go:221] Starting reflector *v1.Etcd (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410095       1 reflector.go:257] Listing and watching *v1.Etcd from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410216       1 reflector.go:221] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410221       1 reflector.go:257] Listing and watching *v1.Secret from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410324       1 reflector.go:221] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410329       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410423       1 reflector.go:221] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410427       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410510       1 reflector.go:221] Starting reflector *v1.Pod (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410515       1 reflector.go:257] Listing and watching *v1.Pod from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410607       1 reflector.go:221] Starting reflector *v1.Service (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410613       1 reflector.go:257] Listing and watching *v1.Service from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410698       1 reflector.go:221] Starting reflector *v1.Deployment (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410703       1 reflector.go:257] Listing and watching *v1.Deployment from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410788       1 reflector.go:221] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410793       1 reflector.go:257] Listing and watching *v1.Secret from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410912       1 reflector.go:221] Starting reflector *v1.Endpoints (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.410917       1 reflector.go:257] Listing and watching *v1.Endpoints from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411030       1 reflector.go:221] Starting reflector *v1.Namespace (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411035       1 reflector.go:257] Listing and watching *v1.Namespace from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411119       1 reflector.go:221] Starting reflector *v1.ServiceAccount (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411124       1 reflector.go:257] Listing and watching *v1.ServiceAccount from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411208       1 reflector.go:221] Starting reflector *v1.PodDisruptionBudget (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411214       1 reflector.go:257] Listing and watching *v1.PodDisruptionBudget from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411304       1 reflector.go:221] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411309       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411344       1 reflector.go:221] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411348       1 reflector.go:257] Listing and watching *v1.Secret from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411383       1 reflector.go:221] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411387       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411422       1 reflector.go:221] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411426       1 reflector.go:257] Listing and watching *v1.Secret from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411458       1 reflector.go:221] Starting reflector *v1.Node (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411462       1 reflector.go:257] Listing and watching *v1.Node from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411493       1 reflector.go:221] Starting reflector *v1.ClusterRoleBinding (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411497       1 reflector.go:257] Listing and watching *v1.ClusterRoleBinding from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411579       1 reflector.go:221] Starting reflector *v1.Namespace (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411584       1 reflector.go:257] Listing and watching *v1.Namespace from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411623       1 reflector.go:221] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.411627       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412319199Z I0504 16:36:05.412112       1 base_controller.go:67] Waiting for caches to sync for EtcdMembersController
2023-05-04T16:36:05.412319199Z I0504 16:36:05.412121       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.412319199Z I0504 16:36:05.412124       1 base_controller.go:73] Caches are synced for EtcdMembersController 
2023-05-04T16:36:05.412319199Z I0504 16:36:05.412128       1 base_controller.go:110] Starting #1 worker of EtcdMembersController controller ...
2023-05-04T16:36:05.412381074Z I0504 16:36:05.412171       1 reflector.go:221] Starting reflector *v1.Network (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412381074Z I0504 16:36:05.412176       1 reflector.go:257] Listing and watching *v1.Network from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412381074Z I0504 16:36:05.412302       1 reflector.go:221] Starting reflector *v1.APIServer (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412381074Z I0504 16:36:05.412307       1 reflector.go:257] Listing and watching *v1.APIServer from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412426499Z I0504 16:36:05.412414       1 reflector.go:221] Starting reflector *v1.Infrastructure (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412426499Z I0504 16:36:05.412421       1 reflector.go:257] Listing and watching *v1.Infrastructure from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412456736Z I0504 16:36:05.412447       1 base_controller.go:67] Waiting for caches to sync for StaticPodStateController
2023-05-04T16:36:05.412501149Z I0504 16:36:05.412493       1 base_controller.go:67] Waiting for caches to sync for BootstrapTeardownController
2023-05-04T16:36:05.412534491Z I0504 16:36:05.412526       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2023-05-04T16:36:05.412559117Z I0504 16:36:05.412549       1 reflector.go:205] Reflector from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169 configured with expectedType of *unstructured.Unstructured with empty GroupVersionKind.
2023-05-04T16:36:05.412565830Z I0504 16:36:05.412561       1 reflector.go:221] Starting reflector *unstructured.Unstructured (12h0m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412565830Z I0504 16:36:05.412564       1 reflector.go:257] Listing and watching *unstructured.Unstructured from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412592981Z I0504 16:36:05.412585       1 base_controller.go:67] Waiting for caches to sync for ScriptController
2023-05-04T16:36:05.412625592Z I0504 16:36:05.412612       1 base_controller.go:67] Waiting for caches to sync for QuorumGuardCleanupController
2023-05-04T16:36:05.412667550Z I0504 16:36:05.412660       1 base_controller.go:67] Waiting for caches to sync for DefragController
2023-05-04T16:36:05.412698478Z I0504 16:36:05.412691       1 base_controller.go:67] Waiting for caches to sync for ClusterBackupController
2023-05-04T16:36:05.412729566Z I0504 16:36:05.412719       1 reflector.go:221] Starting reflector *v1.Node (1h0m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412729566Z I0504 16:36:05.412725       1 reflector.go:257] Listing and watching *v1.Node from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.412751607Z I0504 16:36:05.412745       1 envvarcontroller.go:172] Starting EnvVarController
2023-05-04T16:36:05.412769751Z I0504 16:36:05.412760       1 base_controller.go:67] Waiting for caches to sync for RemoveStaleConditionsController
2023-05-04T16:36:05.412791201Z I0504 16:36:05.412779       1 base_controller.go:67] Waiting for caches to sync for FSyncController
2023-05-04T16:36:05.412791201Z I0504 16:36:05.412784       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.412791201Z I0504 16:36:05.412787       1 base_controller.go:73] Caches are synced for FSyncController 
2023-05-04T16:36:05.412798866Z I0504 16:36:05.412790       1 base_controller.go:110] Starting #1 worker of FSyncController controller ...
2023-05-04T16:36:05.412831908Z I0504 16:36:05.412824       1 base_controller.go:67] Waiting for caches to sync for RevisionController
2023-05-04T16:36:05.412943296Z I0504 16:36:05.412932       1 base_controller.go:67] Waiting for caches to sync for EtcdStaticResources
2023-05-04T16:36:05.413019709Z I0504 16:36:05.413010       1 base_controller.go:67] Waiting for caches to sync for InstallerController
2023-05-04T16:36:05.413063791Z I0504 16:36:05.413056       1 base_controller.go:67] Waiting for caches to sync for InstallerStateController
2023-05-04T16:36:05.413089499Z I0504 16:36:05.412507       1 reflector.go:221] Starting reflector *v1.ClusterOperator (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.413116550Z I0504 16:36:05.413110       1 reflector.go:257] Listing and watching *v1.ClusterOperator from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.413235643Z I0504 16:36:05.413221       1 base_controller.go:67] Waiting for caches to sync for TargetConfigController
2023-05-04T16:36:05.413246062Z I0504 16:36:05.413241       1 base_controller.go:67] Waiting for caches to sync for EtcdCertSignerController
2023-05-04T16:36:05.413262583Z I0504 16:36:05.413251       1 base_controller.go:67] Waiting for caches to sync for EtcdEndpointsController
2023-05-04T16:36:05.413268454Z I0504 16:36:05.413263       1 base_controller.go:67] Waiting for caches to sync for ResourceSyncController
2023-05-04T16:36:05.413328055Z I0504 16:36:05.412531       1 reflector.go:221] Starting reflector *v1.ClusterVersion (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.413352611Z I0504 16:36:05.413341       1 reflector.go:257] Listing and watching *v1.ClusterVersion from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.413400411Z I0504 16:36:05.413388       1 base_controller.go:67] Waiting for caches to sync for StatusSyncer_etcd
2023-05-04T16:36:05.413415659Z I0504 16:36:05.413407       1 base_controller.go:67] Waiting for caches to sync for ConfigObserver
2023-05-04T16:36:05.413430056Z I0504 16:36:05.413422       1 base_controller.go:67] Waiting for caches to sync for ClusterMemberController
2023-05-04T16:36:05.413440155Z I0504 16:36:05.413433       1 base_controller.go:67] Waiting for caches to sync for ClusterMemberRemovalController
2023-05-04T16:36:05.413452057Z I0504 16:36:05.413444       1 base_controller.go:67] Waiting for caches to sync for MachineDeletionHooksController
2023-05-04T16:36:05.413540412Z E0504 16:36:05.413522       1 base_controller.go:270] "EtcdMembersController" controller failed to sync "key", err: getting cache client could not retrieve endpoints: node lister not synced
2023-05-04T16:36:05.413564998Z I0504 16:36:05.412705       1 reflector.go:221] Starting reflector *v1beta1.Machine (1h0m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.413586970Z I0504 16:36:05.413577       1 reflector.go:257] Listing and watching *v1beta1.Machine from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-05-04T16:36:05.414057930Z I0504 16:36:05.414037       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "InstallerController" resync interval is set to 0s which might lead to client request throttling
2023-05-04T16:36:05.414096442Z I0504 16:36:05.414089       1 base_controller.go:67] Waiting for caches to sync for LoggingSyncer
2023-05-04T16:36:05.414138200Z I0504 16:36:05.414130       1 base_controller.go:67] Waiting for caches to sync for PruneController
2023-05-04T16:36:05.414184507Z I0504 16:36:05.414170       1 base_controller.go:67] Waiting for caches to sync for NodeController
2023-05-04T16:36:05.414330520Z I0504 16:36:05.414321       1 base_controller.go:67] Waiting for caches to sync for BackingResourceController
2023-05-04T16:36:05.414370234Z I0504 16:36:05.414362       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2023-05-04T16:36:05.414533129Z I0504 16:36:05.414518       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "RevisionController" resync interval is set to 0s which might lead to client request throttling
2023-05-04T16:36:05.414554238Z I0504 16:36:05.414536       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ReportEtcdMembersErrorUpdatingStatus' etcds.operator.openshift.io "cluster" not found
2023-05-04T16:36:05.414567233Z I0504 16:36:05.414556       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "PruneController" resync interval is set to 0s which might lead to client request throttling
2023-05-04T16:36:05.414583293Z I0504 16:36:05.414570       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "NodeController" resync interval is set to 0s which might lead to client request throttling
2023-05-04T16:36:05.414609171Z I0504 16:36:05.414601       1 base_controller.go:67] Waiting for caches to sync for GuardController
2023-05-04T16:36:05.423988481Z E0504 16:36:05.418837       1 base_controller.go:270] "EtcdMembersController" controller failed to sync "key", err: getting cache client could not retrieve endpoints: node lister not synced
2023-05-04T16:36:05.423988481Z I0504 16:36:05.418874       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ReportEtcdMembersErrorUpdatingStatus' etcds.operator.openshift.io "cluster" not found
2023-05-04T16:36:05.430697758Z E0504 16:36:05.430657       1 base_controller.go:270] "EtcdMembersController" controller failed to sync "key", err: getting cache client could not retrieve endpoints: node lister not synced
2023-05-04T16:36:05.434243118Z I0504 16:36:05.434198       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ReportEtcdMembersErrorUpdatingStatus' etcds.operator.openshift.io "cluster" not found
2023-05-04T16:36:05.473159824Z E0504 16:36:05.472662       1 base_controller.go:270] "EtcdMembersController" controller failed to sync "key", err: getting cache client could not retrieve endpoints: node lister not synced
2023-05-04T16:36:05.508727949Z I0504 16:36:05.506218       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.508727949Z I0504 16:36:05.506236       1 base_controller.go:73] Caches are synced for MissingStaticPodController 
2023-05-04T16:36:05.508727949Z I0504 16:36:05.506245       1 base_controller.go:110] Starting #1 worker of MissingStaticPodController controller ...
2023-05-04T16:36:05.512956125Z I0504 16:36:05.512937       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.512994377Z I0504 16:36:05.512985       1 base_controller.go:73] Caches are synced for RevisionController 
2023-05-04T16:36:05.513020566Z I0504 16:36:05.513004       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.513020566Z I0504 16:36:05.513016       1 base_controller.go:73] Caches are synced for StaticPodStateController 
2023-05-04T16:36:05.513029092Z I0504 16:36:05.513023       1 base_controller.go:110] Starting #1 worker of StaticPodStateController controller ...
2023-05-04T16:36:05.513042577Z I0504 16:36:05.513009       1 base_controller.go:110] Starting #1 worker of RevisionController controller ...
2023-05-04T16:36:05.513067734Z I0504 16:36:05.513057       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.513067734Z I0504 16:36:05.513062       1 base_controller.go:73] Caches are synced for InstallerController 
2023-05-04T16:36:05.513067734Z I0504 16:36:05.513065       1 base_controller.go:110] Starting #1 worker of InstallerController controller ...
2023-05-04T16:36:05.513359650Z I0504 16:36:05.513349       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.513379287Z I0504 16:36:05.513373       1 base_controller.go:73] Caches are synced for DefragController 
2023-05-04T16:36:05.513395477Z I0504 16:36:05.513389       1 base_controller.go:110] Starting #1 worker of DefragController controller ...
2023-05-04T16:36:05.513428519Z I0504 16:36:05.513421       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.513445030Z I0504 16:36:05.513439       1 base_controller.go:73] Caches are synced for ClusterBackupController 
2023-05-04T16:36:05.513460459Z I0504 16:36:05.513454       1 base_controller.go:110] Starting #1 worker of ClusterBackupController controller ...
2023-05-04T16:36:05.513499532Z I0504 16:36:05.513485       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.513499532Z I0504 16:36:05.513492       1 base_controller.go:73] Caches are synced for InstallerStateController 
2023-05-04T16:36:05.513499532Z I0504 16:36:05.513496       1 base_controller.go:110] Starting #1 worker of InstallerStateController controller ...
2023-05-04T16:36:05.513538094Z I0504 16:36:05.512949       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.513554895Z I0504 16:36:05.513549       1 base_controller.go:73] Caches are synced for ScriptController 
2023-05-04T16:36:05.513570765Z I0504 16:36:05.513564       1 base_controller.go:110] Starting #1 worker of ScriptController controller ...
2023-05-04T16:36:05.513670211Z I0504 16:36:05.513655       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.513670211Z I0504 16:36:05.513661       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2023-05-04T16:36:05.513670211Z I0504 16:36:05.513665       1 base_controller.go:110] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2023-05-04T16:36:05.514059439Z I0504 16:36:05.514047       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.514059439Z I0504 16:36:05.514055       1 base_controller.go:73] Caches are synced for RemoveStaleConditionsController 
2023-05-04T16:36:05.514069908Z I0504 16:36:05.514061       1 base_controller.go:110] Starting #1 worker of RemoveStaleConditionsController controller ...
2023-05-04T16:36:05.514101547Z I0504 16:36:05.514093       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.514101547Z I0504 16:36:05.514097       1 base_controller.go:73] Caches are synced for EtcdStaticResources 
2023-05-04T16:36:05.514108380Z I0504 16:36:05.514100       1 base_controller.go:110] Starting #1 worker of EtcdStaticResources controller ...
2023-05-04T16:36:05.514147493Z I0504 16:36:05.514139       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.514147493Z I0504 16:36:05.514142       1 base_controller.go:73] Caches are synced for StatusSyncer_etcd 
2023-05-04T16:36:05.514147493Z I0504 16:36:05.514145       1 base_controller.go:110] Starting #1 worker of StatusSyncer_etcd controller ...
2023-05-04T16:36:05.514350192Z E0504 16:36:05.514339       1 base_controller.go:270] "EtcdMembersController" controller failed to sync "key", err: getting cache client could not retrieve endpoints: node lister not synced
2023-05-04T16:36:05.514760860Z I0504 16:36:05.513612       1 defragcontroller.go:103] Defrag controller disabled for non HA cluster topology: SingleReplica
2023-05-04T16:36:05.515039642Z I0504 16:36:05.515023       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-05-04T16:36:05Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nEtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced","reason":"BackingResourceController_SyncError::EtcdMembersController_ErrorUpdatingReportEtcdMembers","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-05-02T12:38:56Z","message":"NodeInstallerProgressing: 1 nodes are at revision 3\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-05-02T12:31:30Z","message":"StaticPodsAvailable: 1 nodes are active; 1 nodes are at revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-05-04T16:36:05.515293076Z I0504 16:36:05.515278       1 shared_informer.go:285] caches populated
2023-05-04T16:36:05.515293076Z I0504 16:36:05.515284       1 base_controller.go:73] Caches are synced for LoggingSyncer 
2023-05-04T16:36:05.515293076Z I0504 16:36:05.515287       1 base_controller.go:110] Starting #1 worker of LoggingSyncer controller ...
2023-05-04T16:36:05.515487820Z I0504 16:36:05.515475       1 base_controller.go:73] Caches are synced for PruneController 
2023-05-04T16:36:05.515487820Z I0504 16:36:05.515482       1 base_controller.go:110] Starting #1 worker of PruneController controller ...
2023-05-04T16:36:05.515620117Z I0504 16:36:05.515605       1 base_controller.go:73] Caches are synced for BackingResourceController 
2023-05-04T16:36:05.515620117Z I0504 16:36:05.515614       1 base_controller.go:110] Starting #1 worker of BackingResourceController controller ...
2023-05-04T16:36:05.515627741Z I0504 16:36:05.515615       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorLogLevelChange' Operator log level changed from "Debug" to "Normal"
2023-05-04T16:36:05.515633001Z I0504 16:36:05.515628       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2023-05-04T16:36:05.515638151Z I0504 16:36:05.515633       1 base_controller.go:110] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2023-05-04T16:36:05.515643992Z I0504 16:36:05.515606       1 prune_controller.go:269] Nothing to prune
2023-05-04T16:36:05.523975953Z I0504 16:36:05.523932       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded changed from False to True ("BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nEtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced")
2023-05-04T16:36:05.524221102Z I0504 16:36:05.524210       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-05-04T16:36:05Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nEtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced","reason":"BackingResourceController_SyncError::EtcdMembersController_ErrorUpdatingReportEtcdMembers","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-05-02T12:38:56Z","message":"NodeInstallerProgressing: 1 nodes are at revision 3\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-05-02T12:31:30Z","message":"StaticPodsAvailable: 1 nodes are active; 1 nodes are at revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-05-04T16:36:05.526730234Z E0504 16:36:05.526654       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-05-04T16:36:05.529904219Z I0504 16:36:05.529742       1 prune_controller.go:269] Nothing to prune
2023-05-04T16:36:05.538730415Z E0504 16:36:05.532686       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-05-04T16:36:05.538730415Z E0504 16:36:05.532925       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-05-04T16:36:05.541296994Z E0504 16:36:05.540543       1 base_controller.go:272] StatusSyncer_etcd reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "etcd": the object has been modified; please apply your changes to the latest version and try again
2023-05-04T16:36:05.541296994Z I0504 16:36:05.541129       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-05-04T16:36:05Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nEtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values","reason":"BackingResourceController_SyncError::EtcdMembersController_ErrorUpdatingReportEtcdMembers::ScriptController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-05-02T12:38:56Z","message":"NodeInstallerProgressing: 1 nodes are at revision 3\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-05-02T12:31:30Z","message":"StaticPodsAvailable: 1 nodes are active; 1 nodes are at revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-05-04T16:36:05.549552633Z I0504 16:36:05.549518       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-05-04T16:36:05Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nEtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values","reason":"BackingResourceController_SyncError::EtcdMembersController_ErrorUpdatingReportEtcdMembers::ScriptController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-05-02T12:38:56Z","message":"NodeInstallerProgressing: 1 nodes are at revision 3\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-05-02T12:31:30Z","message":"StaticPodsAvailable: 1 nodes are active; 1 nodes are at revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-05-04T16:36:05.549800427Z I0504 16:36:05.549781       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nEtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced" to "BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nEtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values"
2023-05-04T16:36:05.554727931Z E0504 16:36:05.554376       1 base_controller.go:272] StatusSyncer_etcd reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "etcd": the object has been modified; please apply your changes to the latest version and try again
2023-05-04T16:36:05.557501157Z E0504 16:36:05.556381       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-05-04T16:36:05.594924982Z E0504 16:36:05.594891       1 base_controller.go:272] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-05-04T16:36:05.610547888Z E0504 16:36:05.610508       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-05-04T16:36:05.691786969Z E0504 16:36:05.691755       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-05-04T16:36:05.760170220Z E0504 16:36:05.757140       1 base_controller.go:272] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-05-04T16:36:05.853326908Z E0504 16:36:05.853296       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-05-04T16:36:06.078270155Z E0504 16:36:06.078235       1 base_controller.go:272] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-05-04T16:36:06.113122080Z I0504 16:36:06.113090       1 base_controller.go:73] Caches are synced for BootstrapTeardownController 
2023-05-04T16:36:06.113157857Z I0504 16:36:06.113149       1 base_controller.go:110] Starting #1 worker of BootstrapTeardownController controller ...
2023-05-04T16:36:06.113911437Z I0504 16:36:06.113897       1 base_controller.go:73] Caches are synced for MachineDeletionHooksController 
2023-05-04T16:36:06.113911437Z I0504 16:36:06.113907       1 base_controller.go:110] Starting #1 worker of MachineDeletionHooksController controller ...
2023-05-04T16:36:06.122757349Z E0504 16:36:06.122329       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: failed to get bootstrap scaling strategy: failed to get openshift-etcd namespace: namespace "openshift-etcd" not found
2023-05-04T16:36:06.123462237Z I0504 16:36:06.123437       1 prune_controller.go:269] Nothing to prune
2023-05-04T16:36:06.123916607Z E0504 16:36:06.123905       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: failed to get bootstrap scaling strategy: failed to get openshift-etcd namespace: namespace "openshift-etcd" not found
2023-05-04T16:36:06.124230444Z E0504 16:36:06.124212       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-05-04T16:36:06.125224793Z I0504 16:36:06.125200       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-05-04T16:36:05Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nBootstrapTeardownDegraded: failed to get bootstrap scaling strategy: failed to get openshift-etcd namespace: namespace \"openshift-etcd\" not found\nEtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values","reason":"BackingResourceController_SyncError::BootstrapTeardown_Error::EtcdMembersController_ErrorUpdatingReportEtcdMembers::ScriptController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-05-02T12:38:56Z","message":"NodeInstallerProgressing: 1 nodes are at revision 3\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-05-02T12:31:30Z","message":"StaticPodsAvailable: 1 nodes are active; 1 nodes are at revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-05-04T16:36:06.133025120Z I0504 16:36:06.132933       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nEtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values" to "BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nBootstrapTeardownDegraded: failed to get bootstrap scaling strategy: failed to get openshift-etcd namespace: namespace \"openshift-etcd\" not found\nEtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values"
2023-05-04T16:36:06.133116331Z E0504 16:36:06.133106       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: failed to get bootstrap scaling strategy: failed to get openshift-etcd namespace: namespace "openshift-etcd" not found
2023-05-04T16:36:06.153769153Z E0504 16:36:06.153723       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: failed to get bootstrap scaling strategy: failed to get openshift-etcd namespace: namespace "openshift-etcd" not found
2023-05-04T16:36:06.174051422Z E0504 16:36:06.174000       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-05-04T16:36:06.194284940Z E0504 16:36:06.194253       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: failed to get bootstrap scaling strategy: failed to get openshift-etcd namespace: namespace "openshift-etcd" not found
2023-05-04T16:36:06.275952022Z E0504 16:36:06.275918       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: failed to get bootstrap scaling strategy: failed to get openshift-etcd namespace: namespace "openshift-etcd" not found
2023-05-04T16:36:06.413133309Z I0504 16:36:06.413088       1 envvarcontroller.go:178] caches synced
2023-05-04T16:36:06.413444040Z I0504 16:36:06.413431       1 base_controller.go:73] Caches are synced for EtcdCertSignerController 
2023-05-04T16:36:06.413464859Z I0504 16:36:06.413454       1 base_controller.go:73] Caches are synced for TargetConfigController 
2023-05-04T16:36:06.413471170Z I0504 16:36:06.413462       1 base_controller.go:110] Starting #1 worker of TargetConfigController controller ...
2023-05-04T16:36:06.413495877Z I0504 16:36:06.413488       1 base_controller.go:110] Starting #1 worker of EtcdCertSignerController controller ...
2023-05-04T16:36:06.413528778Z I0504 16:36:06.413432       1 base_controller.go:73] Caches are synced for EtcdEndpointsController 
2023-05-04T16:36:06.413546772Z I0504 16:36:06.413540       1 base_controller.go:110] Starting #1 worker of EtcdEndpointsController controller ...
2023-05-04T16:36:06.413913608Z E0504 16:36:06.413863       1 base_controller.go:272] TargetConfigController reconciliation failed: TargetConfigController missing env var values
2023-05-04T16:36:06.413923246Z I0504 16:36:06.413917       1 etcdcli_pool.go:70] creating a new cached client
2023-05-04T16:36:06.414317603Z I0504 16:36:06.414299       1 base_controller.go:73] Caches are synced for NodeController 
2023-05-04T16:36:06.414317603Z I0504 16:36:06.414309       1 base_controller.go:110] Starting #1 worker of NodeController controller ...
2023-05-04T16:36:06.414650836Z I0504 16:36:06.414641       1 base_controller.go:73] Caches are synced for GuardController 
2023-05-04T16:36:06.414650836Z I0504 16:36:06.414648       1 base_controller.go:110] Starting #1 worker of GuardController controller ...
2023-05-04T16:36:06.438107091Z E0504 16:36:06.437890       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: failed to get bootstrap scaling strategy: failed to get openshift-etcd namespace: namespace "openshift-etcd" not found
2023-05-04T16:36:06.611449561Z I0504 16:36:06.611417       1 request.go:601] Waited for 1.199763737s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0
2023-05-04T16:36:06.736251488Z I0504 16:36:06.736214       1 etcdcli_pool.go:70] creating a new cached client
2023-05-04T16:36:06.736909949Z I0504 16:36:06.736890       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-05-04T16:36:05Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nBootstrapTeardownDegraded: failed to get bootstrap scaling strategy: failed to get openshift-etcd namespace: namespace \"openshift-etcd\" not found\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values","reason":"BackingResourceController_SyncError::BootstrapTeardown_Error::ScriptController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-05-02T12:38:56Z","message":"NodeInstallerProgressing: 1 nodes are at revision 3\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-05-02T12:31:30Z","message":"StaticPodsAvailable: 1 nodes are active; 1 nodes are at revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-05-04T16:36:06.739701139Z I0504 16:36:06.737269       1 prune_controller.go:269] Nothing to prune
2023-05-04T16:36:06.748311721Z I0504 16:36:06.745407       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nBootstrapTeardownDegraded: failed to get bootstrap scaling strategy: failed to get openshift-etcd namespace: namespace \"openshift-etcd\" not found\nEtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values" to "BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nBootstrapTeardownDegraded: failed to get bootstrap scaling strategy: failed to get openshift-etcd namespace: namespace \"openshift-etcd\" not found\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values"
2023-05-04T16:36:06.757848706Z I0504 16:36:06.757817       1 prune_controller.go:269] Nothing to prune
2023-05-04T16:36:06.758223237Z I0504 16:36:06.758210       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-05-04T16:36:05Z","message":"BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values","reason":"BackingResourceController_SyncError::ScriptController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-05-02T12:38:56Z","message":"NodeInstallerProgressing: 1 nodes are at revision 3\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-05-02T12:31:30Z","message":"StaticPodsAvailable: 1 nodes are active; 1 nodes are at revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-05-04T16:36:06.766340667Z I0504 16:36:06.766299       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nBootstrapTeardownDegraded: failed to get bootstrap scaling strategy: failed to get openshift-etcd namespace: namespace \"openshift-etcd\" not found\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values" to "BackingResourceControllerDegraded: \"manifests/installer-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \"manifests/installer-cluster-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer\": dial tcp 172.30.0.1:443: i/o timeout\nBackingResourceControllerDegraded: \nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values"
2023-05-04T16:36:06.814168838Z I0504 16:36:06.814133       1 base_controller.go:73] Caches are synced for ResourceSyncController 
2023-05-04T16:36:06.814168838Z I0504 16:36:06.814149       1 base_controller.go:110] Starting #1 worker of ResourceSyncController controller ...
2023-05-04T16:36:06.814196179Z I0504 16:36:06.814171       1 base_controller.go:73] Caches are synced for ConfigObserver 
2023-05-04T16:36:06.814196179Z I0504 16:36:06.814174       1 base_controller.go:110] Starting #1 worker of ConfigObserver controller ...
2023-05-04T16:36:07.113262911Z I0504 16:36:07.113229       1 base_controller.go:73] Caches are synced for QuorumGuardCleanupController 
2023-05-04T16:36:07.113262911Z I0504 16:36:07.113245       1 base_controller.go:110] Starting #1 worker of QuorumGuardCleanupController controller ...
2023-05-04T16:36:07.113559125Z I0504 16:36:07.113548       1 base_controller.go:73] Caches are synced for ClusterMemberRemovalController 
2023-05-04T16:36:07.113559125Z I0504 16:36:07.113556       1 base_controller.go:110] Starting #1 worker of ClusterMemberRemovalController controller ...
2023-05-04T16:36:07.113577059Z I0504 16:36:07.113569       1 base_controller.go:73] Caches are synced for ClusterMemberController 
2023-05-04T16:36:07.113577059Z I0504 16:36:07.113574       1 base_controller.go:110] Starting #1 worker of ClusterMemberController controller ...
2023-05-04T16:36:07.810676415Z I0504 16:36:07.810641       1 request.go:601] Waited for 2.294786994s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-05-04T16:36:08.023343885Z I0504 16:36:08.023308       1 etcdcli_pool.go:70] creating a new cached client
2023-05-04T16:36:08.024775021Z I0504 16:36:08.024751       1 prune_controller.go:269] Nothing to prune
2023-05-04T16:36:08.024787505Z I0504 16:36:08.024759       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-05-04T16:36:08Z","message":"NodeControllerDegraded: All master nodes are ready\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-05-02T12:38:56Z","message":"NodeInstallerProgressing: 1 nodes are at revision 3\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-05-02T12:31:30Z","message":"StaticPodsAvailable: 1 nodes are active; 1 nodes are at revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-05-04T16:36:08.042665217Z I0504 16:36:08.042618       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded changed from True to False ("NodeControllerDegraded: All master nodes are ready\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found")
2023-05-04T16:36:08.240065587Z I0504 16:36:08.239992       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-05-04T16:36:08Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-05-02T12:38:56Z","message":"NodeInstallerProgressing: 1 nodes are at revision 3\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-05-02T12:31:30Z","message":"StaticPodsAvailable: 1 nodes are active; 1 nodes are at revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-05-02T12:28:28Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-05-04T16:36:08.249162268Z I0504 16:36:08.241647       1 prune_controller.go:269] Nothing to prune
2023-05-04T16:36:08.425953638Z I0504 16:36:08.425540       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"cd298360-e02f-46c3-8333-6efee98e158d", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-05-04T16:36:09.010626340Z I0504 16:36:09.010570       1 request.go:601] Waited for 1.197667267s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-05-04T16:36:11.010325081Z I0504 16:36:11.010273       1 request.go:601] Waited for 1.159148064s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-05-04T16:46:06.630188487Z I0504 16:46:06.629939       1 request.go:601] Waited for 1.186184619s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T16:46:07.827510437Z I0504 16:46:07.827479       1 request.go:601] Waited for 1.397404402s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-05-04T16:46:08.828186350Z I0504 16:46:08.828161       1 request.go:601] Waited for 1.197422638s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-05-04T16:56:06.631782896Z I0504 16:56:06.631747       1 request.go:601] Waited for 1.17929601s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-05-04T16:56:07.833072699Z I0504 16:56:07.833032       1 request.go:601] Waited for 1.398120513s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-05-04T17:00:06.734099280Z I0504 17:00:06.727448       1 request.go:601] Waited for 1.192797117s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-05-04T17:06:06.639921174Z I0504 17:06:06.635030       1 request.go:601] Waited for 1.182944958s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T17:06:07.637247045Z I0504 17:06:07.637217       1 request.go:601] Waited for 1.399336669s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-05-04T17:11:06.602609246Z I0504 17:11:06.601835       1 request.go:601] Waited for 1.051018223s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-05-04T17:16:06.634143924Z I0504 17:16:06.634095       1 request.go:601] Waited for 1.183015602s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T17:16:07.830850441Z I0504 17:16:07.830813       1 request.go:601] Waited for 1.397824185s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-05-04T17:16:08.835168527Z I0504 17:16:08.835064       1 request.go:601] Waited for 1.202881211s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-05-04T17:19:06.599072139Z I0504 17:19:06.599006       1 request.go:601] Waited for 1.038077272s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-05-04T17:20:06.710450219Z I0504 17:20:06.710419       1 request.go:601] Waited for 1.131495704s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-05-04T17:26:06.632187613Z I0504 17:26:06.632157       1 request.go:601] Waited for 1.180793863s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T17:26:07.632673460Z I0504 17:26:07.632641       1 request.go:601] Waited for 1.396871668s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-05-04T17:36:06.631646361Z I0504 17:36:06.631606       1 request.go:601] Waited for 1.180025132s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-05-04T17:36:07.831467637Z I0504 17:36:07.831435       1 request.go:601] Waited for 1.397823954s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T17:36:09.031398499Z I0504 17:36:09.031358       1 request.go:601] Waited for 1.197133433s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T17:46:06.633269644Z I0504 17:46:06.632792       1 request.go:601] Waited for 1.180101933s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T17:46:07.832291895Z I0504 17:46:07.832252       1 request.go:601] Waited for 1.392097847s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-05-04T17:56:06.635157793Z I0504 17:56:06.635120       1 request.go:601] Waited for 1.182042778s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T17:56:07.637250422Z I0504 17:56:07.635338       1 request.go:601] Waited for 1.388204277s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-05-04T18:06:06.634931822Z I0504 18:06:06.634885       1 request.go:601] Waited for 1.18098036s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-05-04T18:06:07.635500914Z I0504 18:06:07.635464       1 request.go:601] Waited for 1.394093634s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T18:06:08.835607800Z I0504 18:06:08.835570       1 request.go:601] Waited for 1.196701015s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T18:13:06.797097582Z I0504 18:13:06.797055       1 request.go:601] Waited for 1.172792868s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-05-04T18:16:06.637061369Z I0504 18:16:06.637015       1 request.go:601] Waited for 1.181025424s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-05-04T18:16:07.638722210Z I0504 18:16:07.638682       1 request.go:601] Waited for 1.39823253s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T18:16:08.837473972Z I0504 18:16:08.837432       1 request.go:601] Waited for 1.195087438s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T18:26:06.635538943Z I0504 18:26:06.635494       1 request.go:601] Waited for 1.180022185s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T18:26:07.635763549Z I0504 18:26:07.635727       1 request.go:601] Waited for 1.396351684s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T18:26:08.636977159Z I0504 18:26:08.636936       1 request.go:601] Waited for 1.398598079s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-05-04T18:36:06.637660588Z I0504 18:36:06.637626       1 request.go:601] Waited for 1.182024632s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T18:36:07.637809391Z I0504 18:36:07.637771       1 request.go:601] Waited for 1.397755368s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
2023-05-04T18:46:06.637238862Z I0504 18:46:06.637203       1 request.go:601] Waited for 1.180654531s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-05-04T18:46:07.838238795Z I0504 18:46:07.838026       1 request.go:601] Waited for 1.397707969s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-node
