---
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  creationTimestamp: "2023-05-02T12:45:47Z"
  generation: 1
  labels:
    app.kubernetes.io/component: prometheus
    app.kubernetes.io/instance: k8s
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/part-of: openshift-monitoring
    app.kubernetes.io/version: 2.39.1
  managedFields:
  - apiVersion: monitoring.coreos.com/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:labels:
          .: {}
          f:app.kubernetes.io/component: {}
          f:app.kubernetes.io/instance: {}
          f:app.kubernetes.io/name: {}
          f:app.kubernetes.io/part-of: {}
          f:app.kubernetes.io/version: {}
      f:spec:
        .: {}
        f:additionalAlertRelabelConfigs: {}
        f:alerting:
          .: {}
          f:alertmanagers: {}
        f:arbitraryFSAccessThroughSMs: {}
        f:configMaps: {}
        f:containers: {}
        f:evaluationInterval: {}
        f:externalUrl: {}
        f:image: {}
        f:listenLocal: {}
        f:nodeSelector:
          .: {}
          f:kubernetes.io/os: {}
        f:podMetadata:
          .: {}
          f:annotations:
            .: {}
            f:target.workload.openshift.io/management: {}
          f:labels:
            .: {}
            f:app.kubernetes.io/component: {}
            f:app.kubernetes.io/instance: {}
            f:app.kubernetes.io/name: {}
            f:app.kubernetes.io/part-of: {}
            f:app.kubernetes.io/version: {}
        f:podMonitorNamespaceSelector: {}
        f:podMonitorSelector: {}
        f:priorityClassName: {}
        f:probeNamespaceSelector: {}
        f:probeSelector: {}
        f:replicas: {}
        f:resources:
          .: {}
          f:requests:
            .: {}
            f:cpu: {}
            f:memory: {}
        f:retention: {}
        f:ruleNamespaceSelector: {}
        f:ruleSelector: {}
        f:rules:
          .: {}
          f:alert: {}
        f:scrapeInterval: {}
        f:secrets: {}
        f:securityContext:
          .: {}
          f:fsGroup: {}
          f:runAsNonRoot: {}
          f:runAsUser: {}
        f:serviceAccountName: {}
        f:serviceMonitorNamespaceSelector: {}
        f:serviceMonitorSelector: {}
        f:thanos:
          .: {}
          f:image: {}
          f:resources:
            .: {}
            f:requests:
              .: {}
              f:cpu: {}
              f:memory: {}
          f:version: {}
        f:tsdb: {}
        f:version: {}
        f:volumes: {}
    manager: operator
    operation: Update
    time: "2023-05-02T12:45:47Z"
  - apiVersion: monitoring.coreos.com/v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        .: {}
        f:availableReplicas: {}
        f:conditions:
          .: {}
          k:{"type":"Available"}:
            .: {}
            f:lastTransitionTime: {}
            f:observedGeneration: {}
            f:status: {}
            f:type: {}
          k:{"type":"Reconciled"}:
            .: {}
            f:lastTransitionTime: {}
            f:observedGeneration: {}
            f:status: {}
            f:type: {}
        f:paused: {}
        f:replicas: {}
        f:shardStatuses:
          .: {}
          k:{"shardID":"0"}:
            .: {}
            f:availableReplicas: {}
            f:replicas: {}
            f:shardID: {}
            f:unavailableReplicas: {}
            f:updatedReplicas: {}
        f:unavailableReplicas: {}
        f:updatedReplicas: {}
    manager: PrometheusOperator
    operation: Update
    subresource: status
    time: "2023-05-04T16:33:34Z"
  name: k8s
  namespace: openshift-monitoring
  resourceVersion: "631768"
  uid: c9833c50-a55f-4783-8fa2-c1607fe46161
spec:
  additionalAlertRelabelConfigs:
    key: config.yaml
    name: alert-relabel-configs
    optional: true
  alerting:
    alertmanagers:
    - apiVersion: v2
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      name: alertmanager-main
      namespace: openshift-monitoring
      port: web
      scheme: https
      tlsConfig:
        ca: {}
        caFile: /etc/prometheus/configmaps/serving-certs-ca-bundle/service-ca.crt
        cert: {}
        serverName: alertmanager-main.openshift-monitoring.svc
  arbitraryFSAccessThroughSMs: {}
  configMaps:
  - serving-certs-ca-bundle
  - kubelet-serving-ca-bundle
  - metrics-client-ca
  containers:
  - args:
    - -provider=openshift
    - -https-address=:9091
    - -http-address=
    - -email-domain=*
    - -upstream=http://localhost:9090
    - -openshift-service-account=prometheus-k8s
    - '-openshift-sar={"resource": "namespaces", "verb": "get"}'
    - '-openshift-delegate-urls={"/": {"resource": "namespaces", "verb": "get"}}'
    - -tls-cert=/etc/tls/private/tls.crt
    - -tls-key=/etc/tls/private/tls.key
    - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token
    - -cookie-secret-file=/etc/proxy/secrets/session_secret
    - -openshift-ca=/etc/pki/tls/cert.pem
    - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    env:
    - name: HTTP_PROXY
    - name: HTTPS_PROXY
    - name: NO_PROXY
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:f968922564c3eea1c69d6bbe529d8970784d6cae8935afaf674d9fa7c0f72ea3
    name: prometheus-proxy
    ports:
    - containerPort: 9091
      name: web
      protocol: TCP
    resources:
      requests:
        cpu: 1m
        memory: 20Mi
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/tls/private
      name: secret-prometheus-k8s-tls
    - mountPath: /etc/proxy/secrets
      name: secret-prometheus-k8s-proxy
    - mountPath: /etc/pki/ca-trust/extracted/pem/
      name: prometheus-trusted-ca-bundle
      readOnly: true
  - args:
    - --secure-listen-address=0.0.0.0:9092
    - --upstream=http://127.0.0.1:9090
    - --allow-paths=/metrics
    - --config-file=/etc/kube-rbac-proxy/config.yaml
    - --tls-cert-file=/etc/tls/private/tls.crt
    - --tls-private-key-file=/etc/tls/private/tls.key
    - --client-ca-file=/etc/tls/client/client-ca.crt
    - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
    - --logtostderr=true
    - --tls-min-version=VersionTLS12
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c28f27a3a10df13e5e8c074e8734683a6603ebaccd9d67e2095070fb6859b1d6
    name: kube-rbac-proxy
    ports:
    - containerPort: 9092
      name: metrics
      protocol: TCP
    resources:
      requests:
        cpu: 1m
        memory: 15Mi
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/tls/private
      name: secret-prometheus-k8s-tls
    - mountPath: /etc/tls/client
      name: configmap-metrics-client-ca
      readOnly: true
    - mountPath: /etc/kube-rbac-proxy
      name: secret-kube-rbac-proxy
  - args:
    - --secure-listen-address=[$(POD_IP)]:10902
    - --upstream=http://127.0.0.1:10902
    - --tls-cert-file=/etc/tls/private/tls.crt
    - --tls-private-key-file=/etc/tls/private/tls.key
    - --client-ca-file=/etc/tls/client/client-ca.crt
    - --config-file=/etc/kube-rbac-proxy/config.yaml
    - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
    - --allow-paths=/metrics
    - --logtostderr=true
    - --tls-min-version=VersionTLS12
    - --client-ca-file=/etc/tls/client/client-ca.crt
    env:
    - name: POD_IP
      valueFrom:
        fieldRef:
          fieldPath: status.podIP
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:c28f27a3a10df13e5e8c074e8734683a6603ebaccd9d67e2095070fb6859b1d6
    name: kube-rbac-proxy-thanos
    ports:
    - containerPort: 10902
      name: thanos-proxy
      protocol: TCP
    resources:
      requests:
        cpu: 1m
        memory: 10Mi
    terminationMessagePolicy: FallbackToLogsOnError
    volumeMounts:
    - mountPath: /etc/tls/private
      name: secret-prometheus-k8s-thanos-sidecar-tls
    - mountPath: /etc/kube-rbac-proxy
      name: secret-kube-rbac-proxy
    - mountPath: /etc/tls/client
      name: metrics-client-ca
      readOnly: true
  - args:
    - sidecar
    - --prometheus.url=http://localhost:9090/
    - --tsdb.path=/prometheus
    - --http-address=127.0.0.1:10902
    - --grpc-server-tls-cert=/etc/tls/grpc/server.crt
    - --grpc-server-tls-key=/etc/tls/grpc/server.key
    - --grpc-server-tls-client-ca=/etc/tls/grpc/ca.crt
    name: thanos-sidecar
    resources:
      requests:
        cpu: 1m
        memory: 25Mi
    volumeMounts:
    - mountPath: /etc/tls/grpc
      name: secret-grpc-tls
  - name: prometheus
    resources: {}
    startupProbe:
      failureThreshold: 240
      periodSeconds: 15
    volumeMounts:
    - mountPath: /etc/pki/ca-trust/extracted/pem/
      name: prometheus-trusted-ca-bundle
      readOnly: true
  evaluationInterval: 30s
  externalUrl: https://console-openshift-console.apps.sno.gw.lo/monitoring
  image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:917b84445c725430f74f2041baa697d86d2a0bc971f6b9101591524daf8053f6
  listenLocal: true
  nodeSelector:
    kubernetes.io/os: linux
  podMetadata:
    annotations:
      target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
    labels:
      app.kubernetes.io/component: prometheus
      app.kubernetes.io/instance: k8s
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 2.39.1
  podMonitorNamespaceSelector:
    matchLabels:
      openshift.io/cluster-monitoring: "true"
  podMonitorSelector: {}
  priorityClassName: system-cluster-critical
  probeNamespaceSelector:
    matchLabels:
      openshift.io/cluster-monitoring: "true"
  probeSelector: {}
  replicas: 1
  resources:
    requests:
      cpu: 70m
      memory: 1Gi
  retention: 15d
  ruleNamespaceSelector:
    matchLabels:
      openshift.io/cluster-monitoring: "true"
  ruleSelector: {}
  rules:
    alert: {}
  scrapeInterval: 30s
  secrets:
  - kube-etcd-client-certs
  - prometheus-k8s-tls
  - prometheus-k8s-proxy
  - prometheus-k8s-thanos-sidecar-tls
  - kube-rbac-proxy
  - metrics-client-certs
  securityContext:
    fsGroup: 65534
    runAsNonRoot: true
    runAsUser: 65534
  serviceAccountName: prometheus-k8s
  serviceMonitorNamespaceSelector:
    matchLabels:
      openshift.io/cluster-monitoring: "true"
  serviceMonitorSelector: {}
  thanos:
    image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:00d1be95201020c5cb1d3fae3435ee9e7dc22d8360481ec8609fa368c6ad306e
    resources:
      requests:
        cpu: 1m
        memory: 100Mi
    version: 0.28.1
  tsdb: {}
  version: 2.39.1
  volumes:
  - configMap:
      name: metrics-client-ca
    name: metrics-client-ca
  - name: secret-grpc-tls
    secret:
      secretName: prometheus-k8s-grpc-tls-4brm9jglbl0pb
  - configMap:
      items:
      - key: ca-bundle.crt
        path: tls-ca-bundle.pem
      name: prometheus-trusted-ca-bundle-c7nmestil7q08
      optional: true
    name: prometheus-trusted-ca-bundle
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: "2023-05-04T16:33:34Z"
    observedGeneration: 1
    status: "True"
    type: Available
  - lastTransitionTime: "2023-05-04T16:32:15Z"
    observedGeneration: 1
    status: "True"
    type: Reconciled
  paused: false
  replicas: 1
  shardStatuses:
  - availableReplicas: 1
    replicas: 1
    shardID: "0"
    unavailableReplicas: 0
    updatedReplicas: 1
  unavailableReplicas: 0
  updatedReplicas: 1
