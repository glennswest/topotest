2023-05-04T16:29:13.996980609Z + timeout 3m /bin/bash -exuo pipefail -c 'while [ -n "$(ss -Htanop \( sport = 10357 \))" ]; do sleep 1; done'
2023-05-04T16:29:13.999220935Z ++ ss -Htanop '(' sport = 10357 ')'
2023-05-04T16:29:14.004059840Z + '[' -n '' ']'
2023-05-04T16:29:14.004432579Z + exec cluster-policy-controller start --config=/etc/kubernetes/static-pod-resources/configmaps/cluster-policy-controller-config/config.yaml --kubeconfig=/etc/kubernetes/static-pod-resources/configmaps/controller-manager-kubeconfig/kubeconfig --namespace=openshift-kube-controller-manager -v=2
2023-05-04T16:29:14.037775607Z I0504 16:29:14.037694       1 observer_polling.go:159] Starting file observer
2023-05-04T16:29:14.038166751Z I0504 16:29:14.038152       1 builder.go:262] cluster-policy-controller version 4.12.0-202301171436.p0.g105cc77.assembly.stream-105cc77-105cc773b37f00be2351c9a4e6df24af94d547c1
2023-05-04T16:29:14.038541915Z I0504 16:29:14.038527       1 dynamic_serving_content.go:113] "Loaded a new cert/key pair" name="serving-cert::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key"
2023-05-04T16:29:22.325568179Z W0504 16:29:22.325379       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
2023-05-04T16:29:23.370158717Z I0504 16:29:23.370119       1 requestheader_controller.go:244] Loaded a new request header values for RequestHeaderAuthRequestController
2023-05-04T16:29:23.370894117Z I0504 16:29:23.370878       1 maxinflight.go:140] "Initialized nonMutatingChan" len=400
2023-05-04T16:29:23.370922140Z I0504 16:29:23.370915       1 maxinflight.go:146] "Initialized mutatingChan" len=200
2023-05-04T16:29:23.370956775Z I0504 16:29:23.370949       1 maxinflight.go:117] "Set denominator for readonly requests" limit=400
2023-05-04T16:29:23.370989967Z I0504 16:29:23.370983       1 maxinflight.go:121] "Set denominator for mutating requests" limit=200
2023-05-04T16:29:23.381047038Z I0504 16:29:23.381015       1 genericapiserver.go:480] MuxAndDiscoveryComplete has all endpoints registered and discovery information is complete
2023-05-04T16:29:23.400986823Z W0504 16:29:23.399062       1 builder.go:321] unable to get cluster infrastructure status, using HA cluster values for leader election: infrastructures.config.openshift.io "cluster" is forbidden: User "system:kube-controller-manager" cannot get resource "infrastructures" in API group "config.openshift.io" at the cluster scope
2023-05-04T16:29:23.400986823Z I0504 16:29:23.399335       1 leaderelection.go:248] attempting to acquire leader lease openshift-kube-controller-manager/cluster-policy-controller-lock...
2023-05-04T16:29:23.400986823Z I0504 16:29:23.399577       1 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"openshift-kube-controller-manager", Name:"kube-controller-manager-node", UID:"", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ClusterInfrastructureStatus' unable to get cluster infrastructure status, using HA cluster values for leader election: infrastructures.config.openshift.io "cluster" is forbidden: User "system:kube-controller-manager" cannot get resource "infrastructures" in API group "config.openshift.io" at the cluster scope
2023-05-04T16:29:23.421068283Z I0504 16:29:23.421017       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key" certDetail="\"kube-controller-manager.openshift-kube-controller-manager.svc\" [serving] validServingFor=[kube-controller-manager.openshift-kube-controller-manager.svc,kube-controller-manager.openshift-kube-controller-manager.svc.cluster.local] issuer=\"openshift-service-serving-signer@1683030520\" (2023-05-02 12:31:28 +0000 UTC to 2025-05-01 12:31:29 +0000 UTC (now=2023-05-04 16:29:23.420974156 +0000 UTC))"
2023-05-04T16:29:23.421151629Z I0504 16:29:23.421137       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1683217754\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1683217754\" (2023-05-04 15:29:14 +0000 UTC to 2024-05-03 15:29:14 +0000 UTC (now=2023-05-04 16:29:23.421110482 +0000 UTC))"
2023-05-04T16:29:23.421160256Z I0504 16:29:23.421154       1 secure_serving.go:210] Serving securely on [::]:10357
2023-05-04T16:29:23.421181836Z I0504 16:29:23.421171       1 genericapiserver.go:585] [graceful-termination] waiting for shutdown to be initiated
2023-05-04T16:29:23.421199610Z I0504 16:29:23.421189       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2023-05-04T16:29:23.421219136Z I0504 16:29:23.421209       1 shared_informer.go:255] Waiting for caches to sync for RequestHeaderAuthRequestController
2023-05-04T16:29:23.421248441Z I0504 16:29:23.421236       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key"
2023-05-04T16:29:23.421287354Z I0504 16:29:23.421269       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2023-05-04T16:29:23.421379107Z I0504 16:29:23.421362       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2023-05-04T16:29:23.421379107Z I0504 16:29:23.421371       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2023-05-04T16:29:23.421403592Z I0504 16:29:23.421392       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2023-05-04T16:29:23.421403592Z I0504 16:29:23.421398       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2023-05-04T16:29:23.421486077Z I0504 16:29:23.421476       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2023-05-04T16:29:23.421507037Z I0504 16:29:23.421500       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2023-05-04T16:29:23.522408587Z I0504 16:29:23.522216       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2023-05-04T16:29:23.522408587Z I0504 16:29:23.522274       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2023-05-04T16:29:23.522611468Z I0504 16:29:23.522594       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:54 +0000 UTC to 2033-04-29 12:12:54 +0000 UTC (now=2023-05-04 16:29:23.522572805 +0000 UTC))"
2023-05-04T16:29:23.522632387Z I0504 16:29:23.522621       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:57 +0000 UTC to 2024-05-01 12:12:57 +0000 UTC (now=2023-05-04 16:29:23.5226076 +0000 UTC))"
2023-05-04T16:29:23.522647766Z I0504 16:29:23.522643       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:57 +0000 UTC to 2024-05-01 12:12:57 +0000 UTC (now=2023-05-04 16:29:23.522631255 +0000 UTC))"
2023-05-04T16:29:23.522671611Z I0504 16:29:23.522660       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:55 +0000 UTC to 2033-04-29 12:12:55 +0000 UTC (now=2023-05-04 16:29:23.522648587 +0000 UTC))"
2023-05-04T16:29:23.522690125Z I0504 16:29:23.522680       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1683031055\" [] issuer=\"<self>\" (2023-05-02 12:37:35 +0000 UTC to 2024-05-01 12:37:36 +0000 UTC (now=2023-05-04 16:29:23.522668094 +0000 UTC))"
2023-05-04T16:29:23.522707658Z I0504 16:29:23.522698       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-controller-manager-operator_csr-signer-signer@1683098709\" [] issuer=\"<self>\" (2023-05-03 07:25:09 +0000 UTC to 2023-07-02 07:25:10 +0000 UTC (now=2023-05-04 16:29:23.52268754 +0000 UTC))"
2023-05-04T16:29:23.522727205Z I0504 16:29:23.522718       1 tlsconfig.go:178] "Loaded client CA" index=6 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1683098906\" [] issuer=\"openshift-kube-controller-manager-operator_csr-signer-signer@1683098709\" (2023-05-03 07:28:25 +0000 UTC to 2023-06-02 07:28:26 +0000 UTC (now=2023-05-04 16:29:23.522705424 +0000 UTC))"
2023-05-04T16:29:23.522833194Z I0504 16:29:23.522822       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key" certDetail="\"kube-controller-manager.openshift-kube-controller-manager.svc\" [serving] validServingFor=[kube-controller-manager.openshift-kube-controller-manager.svc,kube-controller-manager.openshift-kube-controller-manager.svc.cluster.local] issuer=\"openshift-service-serving-signer@1683030520\" (2023-05-02 12:31:28 +0000 UTC to 2025-05-01 12:31:29 +0000 UTC (now=2023-05-04 16:29:23.522807516 +0000 UTC))"
2023-05-04T16:29:23.522915448Z I0504 16:29:23.522905       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1683217754\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1683217754\" (2023-05-04 15:29:14 +0000 UTC to 2024-05-03 15:29:14 +0000 UTC (now=2023-05-04 16:29:23.522893247 +0000 UTC))"
2023-05-04T16:29:23.523087541Z I0504 16:29:23.523074       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:54 +0000 UTC to 2033-04-29 12:12:54 +0000 UTC (now=2023-05-04 16:29:23.523059519 +0000 UTC))"
2023-05-04T16:29:23.523099043Z I0504 16:29:23.523095       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:57 +0000 UTC to 2024-05-01 12:12:57 +0000 UTC (now=2023-05-04 16:29:23.523084826 +0000 UTC))"
2023-05-04T16:29:23.523124621Z I0504 16:29:23.523114       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:57 +0000 UTC to 2024-05-01 12:12:57 +0000 UTC (now=2023-05-04 16:29:23.523100075 +0000 UTC))"
2023-05-04T16:29:23.523147344Z I0504 16:29:23.523133       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:55 +0000 UTC to 2033-04-29 12:12:55 +0000 UTC (now=2023-05-04 16:29:23.523121776 +0000 UTC))"
2023-05-04T16:29:23.523153475Z I0504 16:29:23.523150       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1683031055\" [] issuer=\"<self>\" (2023-05-02 12:37:35 +0000 UTC to 2024-05-01 12:37:36 +0000 UTC (now=2023-05-04 16:29:23.523139569 +0000 UTC))"
2023-05-04T16:29:23.523175927Z I0504 16:29:23.523165       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-controller-manager-operator_csr-signer-signer@1683098709\" [] issuer=\"<self>\" (2023-05-03 07:25:09 +0000 UTC to 2023-07-02 07:25:10 +0000 UTC (now=2023-05-04 16:29:23.523155078 +0000 UTC))"
2023-05-04T16:29:23.523192468Z I0504 16:29:23.523183       1 tlsconfig.go:178] "Loaded client CA" index=6 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1683098906\" [] issuer=\"openshift-kube-controller-manager-operator_csr-signer-signer@1683098709\" (2023-05-03 07:28:25 +0000 UTC to 2023-06-02 07:28:26 +0000 UTC (now=2023-05-04 16:29:23.52317218 +0000 UTC))"
2023-05-04T16:29:23.523203629Z I0504 16:29:23.523200       1 tlsconfig.go:178] "Loaded client CA" index=7 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_aggregator-client-signer@1683098704\" [] issuer=\"<self>\" (2023-05-03 07:25:03 +0000 UTC to 2023-06-02 07:25:04 +0000 UTC (now=2023-05-04 16:29:23.523189853 +0000 UTC))"
2023-05-04T16:29:23.523293598Z I0504 16:29:23.523282       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key" certDetail="\"kube-controller-manager.openshift-kube-controller-manager.svc\" [serving] validServingFor=[kube-controller-manager.openshift-kube-controller-manager.svc,kube-controller-manager.openshift-kube-controller-manager.svc.cluster.local] issuer=\"openshift-service-serving-signer@1683030520\" (2023-05-02 12:31:28 +0000 UTC to 2025-05-01 12:31:29 +0000 UTC (now=2023-05-04 16:29:23.523269052 +0000 UTC))"
2023-05-04T16:29:23.523373719Z I0504 16:29:23.523362       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1683217754\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1683217754\" (2023-05-04 15:29:14 +0000 UTC to 2024-05-03 15:29:14 +0000 UTC (now=2023-05-04 16:29:23.523352369 +0000 UTC))"
2023-05-04T16:29:23.523741960Z I0504 16:29:23.523498       1 shared_informer.go:262] Caches are synced for RequestHeaderAuthRequestController
2023-05-04T16:29:23.523741960Z I0504 16:29:23.523635       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2023-05-04T16:29:23.523966311Z I0504 16:29:23.523944       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:54 +0000 UTC to 2033-04-29 12:12:54 +0000 UTC (now=2023-05-04 16:29:23.523930003 +0000 UTC))"
2023-05-04T16:29:23.523978344Z I0504 16:29:23.523974       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:57 +0000 UTC to 2024-05-01 12:12:57 +0000 UTC (now=2023-05-04 16:29:23.523956633 +0000 UTC))"
2023-05-04T16:29:23.524001427Z I0504 16:29:23.523990       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:57 +0000 UTC to 2024-05-01 12:12:57 +0000 UTC (now=2023-05-04 16:29:23.523980328 +0000 UTC))"
2023-05-04T16:29:23.524012157Z I0504 16:29:23.524007       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:55 +0000 UTC to 2033-04-29 12:12:55 +0000 UTC (now=2023-05-04 16:29:23.523997279 +0000 UTC))"
2023-05-04T16:29:23.524037154Z I0504 16:29:23.524025       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1683031055\" [] issuer=\"<self>\" (2023-05-02 12:37:35 +0000 UTC to 2024-05-01 12:37:36 +0000 UTC (now=2023-05-04 16:29:23.524012428 +0000 UTC))"
2023-05-04T16:29:23.524055098Z I0504 16:29:23.524045       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-controller-manager-operator_csr-signer-signer@1683098709\" [] issuer=\"<self>\" (2023-05-03 07:25:09 +0000 UTC to 2023-07-02 07:25:10 +0000 UTC (now=2023-05-04 16:29:23.524034139 +0000 UTC))"
2023-05-04T16:29:23.524072310Z I0504 16:29:23.524064       1 tlsconfig.go:178] "Loaded client CA" index=6 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1683098906\" [] issuer=\"openshift-kube-controller-manager-operator_csr-signer-signer@1683098709\" (2023-05-03 07:28:25 +0000 UTC to 2023-06-02 07:28:26 +0000 UTC (now=2023-05-04 16:29:23.524052463 +0000 UTC))"
2023-05-04T16:29:23.524087419Z I0504 16:29:23.524079       1 tlsconfig.go:178] "Loaded client CA" index=7 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:54 +0000 UTC to 2033-04-29 12:12:54 +0000 UTC (now=2023-05-04 16:29:23.524069936 +0000 UTC))"
2023-05-04T16:29:23.524105793Z I0504 16:29:23.524096       1 tlsconfig.go:178] "Loaded client CA" index=8 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:57 +0000 UTC to 2024-05-01 12:12:57 +0000 UTC (now=2023-05-04 16:29:23.524085445 +0000 UTC))"
2023-05-04T16:29:23.524123817Z I0504 16:29:23.524114       1 tlsconfig.go:178] "Loaded client CA" index=9 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:57 +0000 UTC to 2024-05-01 12:12:57 +0000 UTC (now=2023-05-04 16:29:23.524102467 +0000 UTC))"
2023-05-04T16:29:23.524139927Z I0504 16:29:23.524131       1 tlsconfig.go:178] "Loaded client CA" index=10 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2023-05-02 12:12:55 +0000 UTC to 2033-04-29 12:12:55 +0000 UTC (now=2023-05-04 16:29:23.524120932 +0000 UTC))"
2023-05-04T16:29:23.524156619Z I0504 16:29:23.524148       1 tlsconfig.go:178] "Loaded client CA" index=11 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1683031055\" [] issuer=\"<self>\" (2023-05-02 12:37:35 +0000 UTC to 2024-05-01 12:37:36 +0000 UTC (now=2023-05-04 16:29:23.524137272 +0000 UTC))"
2023-05-04T16:29:23.524173580Z I0504 16:29:23.524165       1 tlsconfig.go:178] "Loaded client CA" index=12 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-controller-manager-operator_csr-signer-signer@1683098709\" [] issuer=\"<self>\" (2023-05-03 07:25:09 +0000 UTC to 2023-07-02 07:25:10 +0000 UTC (now=2023-05-04 16:29:23.524154134 +0000 UTC))"
2023-05-04T16:29:23.524191765Z I0504 16:29:23.524181       1 tlsconfig.go:178] "Loaded client CA" index=13 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1683098906\" [] issuer=\"openshift-kube-controller-manager-operator_csr-signer-signer@1683098709\" (2023-05-03 07:28:25 +0000 UTC to 2023-06-02 07:28:26 +0000 UTC (now=2023-05-04 16:29:23.524170715 +0000 UTC))"
2023-05-04T16:29:23.524215279Z I0504 16:29:23.524198       1 tlsconfig.go:178] "Loaded client CA" index=14 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_aggregator-client-signer@1683098704\" [] issuer=\"<self>\" (2023-05-03 07:25:03 +0000 UTC to 2023-06-02 07:25:04 +0000 UTC (now=2023-05-04 16:29:23.524188318 +0000 UTC))"
2023-05-04T16:29:23.524319494Z I0504 16:29:23.524305       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.crt::/etc/kubernetes/static-pod-resources/secrets/serving-cert/tls.key" certDetail="\"kube-controller-manager.openshift-kube-controller-manager.svc\" [serving] validServingFor=[kube-controller-manager.openshift-kube-controller-manager.svc,kube-controller-manager.openshift-kube-controller-manager.svc.cluster.local] issuer=\"openshift-service-serving-signer@1683030520\" (2023-05-02 12:31:28 +0000 UTC to 2025-05-01 12:31:29 +0000 UTC (now=2023-05-04 16:29:23.524290049 +0000 UTC))"
2023-05-04T16:29:23.524717912Z I0504 16:29:23.524391       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1683217754\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1683217754\" (2023-05-04 15:29:14 +0000 UTC to 2024-05-03 15:29:14 +0000 UTC (now=2023-05-04 16:29:23.524366613 +0000 UTC))"
2023-05-04T16:32:29.984070474Z I0504 16:32:29.984032       1 leaderelection.go:258] successfully acquired lease openshift-kube-controller-manager/cluster-policy-controller-lock
2023-05-04T16:32:29.985453559Z I0504 16:32:29.984872       1 event.go:285] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"openshift-kube-controller-manager", Name:"cluster-policy-controller-lock", UID:"380d25dc-0aff-4ab7-a71a-fb27e6bb8afa", APIVersion:"v1", ResourceVersion:"630902", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' node_b3697e14-f007-4a9a-a7c9-3052cf5dd4a5 became leader
2023-05-04T16:32:29.985453559Z I0504 16:32:29.984898       1 event.go:285] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-kube-controller-manager", Name:"cluster-policy-controller-lock", UID:"cbea2c2a-3556-4bb3-9420-00c1a7dbecb3", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"630904", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' node_b3697e14-f007-4a9a-a7c9-3052cf5dd4a5 became leader
2023-05-04T16:32:29.986298946Z I0504 16:32:29.986127       1 policy_controller.go:78] Starting "openshift.io/cluster-quota-reconciliation"
2023-05-04T16:32:30.471586689Z E0504 16:32:30.471434       1 reconciliation_controller.go:118] initial discovery check failure, continuing and counting on future sync update: unable to retrieve the complete list of server APIs: apps.openshift.io/v1: the server is currently unable to handle the request, authorization.openshift.io/v1: the server is currently unable to handle the request, build.openshift.io/v1: the server is currently unable to handle the request, image.openshift.io/v1: the server is currently unable to handle the request, metrics.k8s.io/v1beta1: the server is currently unable to handle the request, oauth.openshift.io/v1: the server is currently unable to handle the request, packages.operators.coreos.com/v1: the server is currently unable to handle the request, project.openshift.io/v1: the server is currently unable to handle the request, quota.openshift.io/v1: the server is currently unable to handle the request, route.openshift.io/v1: the server is currently unable to handle the request, security.openshift.io/v1: the server is currently unable to handle the request, template.openshift.io/v1: the server is currently unable to handle the request, user.openshift.io/v1: the server is currently unable to handle the request
2023-05-04T16:32:30.471586689Z I0504 16:32:30.471479       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for endpointslices.discovery.k8s.io
2023-05-04T16:32:30.471586689Z I0504 16:32:30.471561       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for roles.rbac.authorization.k8s.io
2023-05-04T16:32:30.471613910Z I0504 16:32:30.471593       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for serviceaccounts
2023-05-04T16:32:30.471613910Z I0504 16:32:30.471610       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for daemonsets.apps
2023-05-04T16:32:30.471634779Z I0504 16:32:30.471622       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for statefulsets.apps
2023-05-04T16:32:30.471670746Z I0504 16:32:30.471660       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for ingresses.networking.k8s.io
2023-05-04T16:32:30.471688389Z I0504 16:32:30.471678       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for leases.coordination.k8s.io
2023-05-04T16:32:30.471736630Z I0504 16:32:30.471726       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for cronjobs.batch
2023-05-04T16:32:30.471755635Z I0504 16:32:30.471745       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for replicasets.apps
2023-05-04T16:32:30.471772016Z I0504 16:32:30.471762       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for networkpolicies.networking.k8s.io
2023-05-04T16:32:30.471797614Z I0504 16:32:30.471788       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for deployments.apps
2023-05-04T16:32:30.471834734Z I0504 16:32:30.471822       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for poddisruptionbudgets.policy
2023-05-04T16:32:30.471889777Z I0504 16:32:30.471874       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for controllerrevisions.apps
2023-05-04T16:32:30.471897552Z I0504 16:32:30.471889       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for rolebindings.rbac.authorization.k8s.io
2023-05-04T16:32:30.471998762Z I0504 16:32:30.471985       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for jobs.batch
2023-05-04T16:32:30.472008019Z I0504 16:32:30.471999       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for csistoragecapacities.storage.k8s.io
2023-05-04T16:32:30.472056370Z I0504 16:32:30.472045       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for podtemplates
2023-05-04T16:32:30.472085234Z I0504 16:32:30.472067       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for limitranges
2023-05-04T16:32:30.472135618Z I0504 16:32:30.472098       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for endpoints
2023-05-04T16:32:30.472162208Z I0504 16:32:30.472151       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for horizontalpodautoscalers.autoscaling
2023-05-04T16:32:30.472368666Z E0504 16:32:30.472348       1 reconciliation_controller.go:124] initial monitor sync has error: [couldn't start monitor for resource "lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroups": unable to monitor quota for resource "lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroups", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=podmonitors": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=podmonitors", couldn't start monitor for resource "operators.coreos.com/v1, Resource=operatorgroups": unable to monitor quota for resource "operators.coreos.com/v1, Resource=operatorgroups", couldn't start monitor for resource "autoscaling.openshift.io/v1beta1, Resource=machineautoscalers": unable to monitor quota for resource "autoscaling.openshift.io/v1beta1, Resource=machineautoscalers", couldn't start monitor for resource "snapshot.storage.k8s.io/v1, Resource=volumesnapshots": unable to monitor quota for resource "snapshot.storage.k8s.io/v1, Resource=volumesnapshots", couldn't start monitor for resource "tuned.openshift.io/v1, Resource=tuneds": unable to monitor quota for resource "tuned.openshift.io/v1, Resource=tuneds", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machinehealthchecks": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machinehealthchecks", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=probes": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=probes", couldn't start monitor for resource "monitoring.coreos.com/v1beta1, Resource=alertmanagerconfigs": unable to monitor quota for resource "monitoring.coreos.com/v1beta1, Resource=alertmanagerconfigs", couldn't start monitor for resource "cloudcredential.openshift.io/v1, Resource=credentialsrequests": unable to monitor quota for resource "cloudcredential.openshift.io/v1, Resource=credentialsrequests", couldn't start monitor for resource "k8s.ovn.org/v1, Resource=egressqoses": unable to monitor quota for resource "k8s.ovn.org/v1, Resource=egressqoses", couldn't start monitor for resource "network.operator.openshift.io/v1, Resource=egressrouters": unable to monitor quota for resource "network.operator.openshift.io/v1, Resource=egressrouters", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=clusterserviceversions": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=clusterserviceversions", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=preprovisioningimages": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=preprovisioningimages", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=hardwaredata": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=hardwaredata", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=catalogsources": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=catalogsources", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=hostfirmwaresettings": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=hostfirmwaresettings", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machines": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machines", couldn't start monitor for resource "lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroupnodestatuses": unable to monitor quota for resource "lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroupnodestatuses", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=firmwareschemas": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=firmwareschemas", couldn't start monitor for resource "lvm.topolvm.io/v1alpha1, Resource=lvmclusters": unable to monitor quota for resource "lvm.topolvm.io/v1alpha1, Resource=lvmclusters", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=bmceventsubscriptions": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=bmceventsubscriptions", couldn't start monitor for resource "machine.openshift.io/v1, Resource=controlplanemachinesets": unable to monitor quota for resource "machine.openshift.io/v1, Resource=controlplanemachinesets", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machinesets": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machinesets", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=thanosrulers": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=thanosrulers", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=prometheuses": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=prometheuses", couldn't start monitor for resource "operator.openshift.io/v1, Resource=ingresscontrollers": unable to monitor quota for resource "operator.openshift.io/v1, Resource=ingresscontrollers", couldn't start monitor for resource "k8s.cni.cncf.io/v1, Resource=network-attachment-definitions": unable to monitor quota for resource "k8s.cni.cncf.io/v1, Resource=network-attachment-definitions", couldn't start monitor for resource "network.operator.openshift.io/v1, Resource=operatorpkis": unable to monitor quota for resource "network.operator.openshift.io/v1, Resource=operatorpkis", couldn't start monitor for resource "tuned.openshift.io/v1, Resource=profiles": unable to monitor quota for resource "tuned.openshift.io/v1, Resource=profiles", couldn't start monitor for resource "controlplane.operator.openshift.io/v1alpha1, Resource=podnetworkconnectivitychecks": unable to monitor quota for resource "controlplane.operator.openshift.io/v1alpha1, Resource=podnetworkconnectivitychecks", couldn't start monitor for resource "ingress.operator.openshift.io/v1, Resource=dnsrecords": unable to monitor quota for resource "ingress.operator.openshift.io/v1, Resource=dnsrecords", couldn't start monitor for resource "k8s.ovn.org/v1, Resource=egressfirewalls": unable to monitor quota for resource "k8s.ovn.org/v1, Resource=egressfirewalls", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=servicemonitors": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=servicemonitors", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=prometheusrules": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=prometheusrules", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=alertmanagers": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=alertmanagers", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=installplans": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=installplans", couldn't start monitor for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=ippools": unable to monitor quota for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=ippools", couldn't start monitor for resource "operators.coreos.com/v2, Resource=operatorconditions": unable to monitor quota for resource "operators.coreos.com/v2, Resource=operatorconditions", couldn't start monitor for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=overlappingrangeipreservations": unable to monitor quota for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=overlappingrangeipreservations", couldn't start monitor for resource "helm.openshift.io/v1beta1, Resource=projecthelmchartrepositories": unable to monitor quota for resource "helm.openshift.io/v1beta1, Resource=projecthelmchartrepositories", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=subscriptions": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=subscriptions", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=baremetalhosts": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=baremetalhosts"]
2023-05-04T16:32:30.473490491Z I0504 16:32:30.473467       1 reconciliation_controller.go:137] Starting the cluster quota reconciliation controller
2023-05-04T16:32:30.473504136Z I0504 16:32:30.473464       1 policy_controller.go:88] Started "openshift.io/cluster-quota-reconciliation"
2023-05-04T16:32:30.473504136Z I0504 16:32:30.473494       1 policy_controller.go:78] Starting "openshift.io/cluster-csr-approver"
2023-05-04T16:32:30.473616607Z I0504 16:32:30.473602       1 resource_quota_monitor.go:295] QuotaMonitor running
2023-05-04T16:32:30.473624582Z I0504 16:32:30.473620       1 clusterquotamapping.go:127] Starting ClusterQuotaMappingController controller
2023-05-04T16:32:30.496913981Z E0504 16:32:30.496826       1 reconciliation_controller.go:167] unable to retrieve the complete list of server APIs: apps.openshift.io/v1: the server is currently unable to handle the request, authorization.openshift.io/v1: the server is currently unable to handle the request, build.openshift.io/v1: the server is currently unable to handle the request, image.openshift.io/v1: the server is currently unable to handle the request, metrics.k8s.io/v1beta1: the server is currently unable to handle the request, oauth.openshift.io/v1: the server is currently unable to handle the request, packages.operators.coreos.com/v1: the server is currently unable to handle the request, project.openshift.io/v1: the server is currently unable to handle the request, quota.openshift.io/v1: the server is currently unable to handle the request, route.openshift.io/v1: the server is currently unable to handle the request, security.openshift.io/v1: the server is currently unable to handle the request, template.openshift.io/v1: the server is currently unable to handle the request, user.openshift.io/v1: the server is currently unable to handle the request
2023-05-04T16:32:30.496944858Z I0504 16:32:30.496860       1 reconciliation_controller.go:203] syncing resource quota controller with updated resources from discovery: map[/v1, Resource=configmaps:{} /v1, Resource=endpoints:{} /v1, Resource=events:{} /v1, Resource=limitranges:{} /v1, Resource=persistentvolumeclaims:{} /v1, Resource=pods:{} /v1, Resource=podtemplates:{} /v1, Resource=replicationcontrollers:{} /v1, Resource=resourcequotas:{} /v1, Resource=secrets:{} /v1, Resource=serviceaccounts:{} /v1, Resource=services:{} apps/v1, Resource=controllerrevisions:{} apps/v1, Resource=daemonsets:{} apps/v1, Resource=deployments:{} apps/v1, Resource=replicasets:{} apps/v1, Resource=statefulsets:{} autoscaling/v2, Resource=horizontalpodautoscalers:{} autoscaling.openshift.io/v1beta1, Resource=machineautoscalers:{} batch/v1, Resource=cronjobs:{} batch/v1, Resource=jobs:{} cloudcredential.openshift.io/v1, Resource=credentialsrequests:{} controlplane.operator.openshift.io/v1alpha1, Resource=podnetworkconnectivitychecks:{} coordination.k8s.io/v1, Resource=leases:{} discovery.k8s.io/v1, Resource=endpointslices:{} events.k8s.io/v1, Resource=events:{} helm.openshift.io/v1beta1, Resource=projecthelmchartrepositories:{} ingress.operator.openshift.io/v1, Resource=dnsrecords:{} k8s.cni.cncf.io/v1, Resource=network-attachment-definitions:{} k8s.ovn.org/v1, Resource=egressfirewalls:{} k8s.ovn.org/v1, Resource=egressqoses:{} lvm.topolvm.io/v1alpha1, Resource=lvmclusters:{} lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroupnodestatuses:{} lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroups:{} machine.openshift.io/v1, Resource=controlplanemachinesets:{} machine.openshift.io/v1beta1, Resource=machinehealthchecks:{} machine.openshift.io/v1beta1, Resource=machines:{} machine.openshift.io/v1beta1, Resource=machinesets:{} metal3.io/v1alpha1, Resource=baremetalhosts:{} metal3.io/v1alpha1, Resource=bmceventsubscriptions:{} metal3.io/v1alpha1, Resource=firmwareschemas:{} metal3.io/v1alpha1, Resource=hardwaredata:{} metal3.io/v1alpha1, Resource=hostfirmwaresettings:{} metal3.io/v1alpha1, Resource=preprovisioningimages:{} monitoring.coreos.com/v1, Resource=alertmanagers:{} monitoring.coreos.com/v1, Resource=podmonitors:{} monitoring.coreos.com/v1, Resource=probes:{} monitoring.coreos.com/v1, Resource=prometheuses:{} monitoring.coreos.com/v1, Resource=prometheusrules:{} monitoring.coreos.com/v1, Resource=servicemonitors:{} monitoring.coreos.com/v1, Resource=thanosrulers:{} monitoring.coreos.com/v1beta1, Resource=alertmanagerconfigs:{} network.operator.openshift.io/v1, Resource=egressrouters:{} network.operator.openshift.io/v1, Resource=operatorpkis:{} networking.k8s.io/v1, Resource=ingresses:{} networking.k8s.io/v1, Resource=networkpolicies:{} operator.openshift.io/v1, Resource=ingresscontrollers:{} operators.coreos.com/v1, Resource=operatorgroups:{} operators.coreos.com/v1alpha1, Resource=catalogsources:{} operators.coreos.com/v1alpha1, Resource=clusterserviceversions:{} operators.coreos.com/v1alpha1, Resource=installplans:{} operators.coreos.com/v1alpha1, Resource=subscriptions:{} operators.coreos.com/v2, Resource=operatorconditions:{} policy/v1, Resource=poddisruptionbudgets:{} rbac.authorization.k8s.io/v1, Resource=rolebindings:{} rbac.authorization.k8s.io/v1, Resource=roles:{} snapshot.storage.k8s.io/v1, Resource=volumesnapshots:{} storage.k8s.io/v1, Resource=csistoragecapacities:{} tuned.openshift.io/v1, Resource=profiles:{} tuned.openshift.io/v1, Resource=tuneds:{} whereabouts.cni.cncf.io/v1alpha1, Resource=ippools:{} whereabouts.cni.cncf.io/v1alpha1, Resource=overlappingrangeipreservations:{}]
2023-05-04T16:32:30.672907530Z I0504 16:32:30.672842       1 policy_controller.go:88] Started "openshift.io/cluster-csr-approver"
2023-05-04T16:32:30.672907530Z I0504 16:32:30.672860       1 policy_controller.go:78] Starting "openshift.io/podsecurity-admission-label-syncer"
2023-05-04T16:32:30.673111082Z I0504 16:32:30.672952       1 base_controller.go:67] Waiting for caches to sync for WebhookAuthenticatorCertApprover_csr-approver-controller
2023-05-04T16:32:30.676792079Z I0504 16:32:30.676760       1 policy_controller.go:88] Started "openshift.io/podsecurity-admission-label-syncer"
2023-05-04T16:32:30.676792079Z I0504 16:32:30.676771       1 policy_controller.go:78] Starting "openshift.io/namespace-security-allocation"
2023-05-04T16:32:30.678720687Z I0504 16:32:30.677073       1 base_controller.go:67] Waiting for caches to sync for pod-security-admission-label-synchronization-controller
2023-05-04T16:32:30.678720687Z I0504 16:32:30.677089       1 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"openshift-kube-controller-manager", Name:"kube-controller-manager-node", UID:"", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "pod-security-admission-label-synchronization-controller" resync interval is set to 0s which might lead to client request throttling
2023-05-04T16:32:30.853799931Z I0504 16:32:30.853764       1 policy_controller.go:88] Started "openshift.io/namespace-security-allocation"
2023-05-04T16:32:30.853799931Z I0504 16:32:30.853779       1 policy_controller.go:78] Starting "openshift.io/resourcequota"
2023-05-04T16:32:30.854071982Z I0504 16:32:30.854050       1 base_controller.go:67] Waiting for caches to sync for namespace-security-allocation-controller
2023-05-04T16:32:30.854084916Z I0504 16:32:30.854069       1 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"openshift-kube-controller-manager", Name:"kube-controller-manager-node", UID:"", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "namespace-security-allocation-controller" resync interval is set to 0s which might lead to client request throttling
2023-05-04T16:32:31.103895229Z I0504 16:32:31.103840       1 policy_controller.go:88] Started "openshift.io/resourcequota"
2023-05-04T16:32:31.103895229Z I0504 16:32:31.103866       1 policy_controller.go:91] Started Origin Controllers
2023-05-04T16:32:31.109651510Z I0504 16:32:31.103854       1 resource_quota_controller.go:277] Starting resource quota controller
2023-05-04T16:32:31.109651510Z I0504 16:32:31.109632       1 shared_informer.go:255] Waiting for caches to sync for resource quota
2023-05-04T16:32:31.110503528Z I0504 16:32:31.110479       1 resource_quota_monitor.go:295] QuotaMonitor running
2023-05-04T16:32:31.116293973Z E0504 16:32:31.113918       1 reconciliation_controller.go:213] failed to sync resource monitors: [couldn't start monitor for resource "tuned.openshift.io/v1, Resource=profiles": unable to monitor quota for resource "tuned.openshift.io/v1, Resource=profiles", couldn't start monitor for resource "tuned.openshift.io/v1, Resource=tuneds": unable to monitor quota for resource "tuned.openshift.io/v1, Resource=tuneds", couldn't start monitor for resource "lvm.topolvm.io/v1alpha1, Resource=lvmclusters": unable to monitor quota for resource "lvm.topolvm.io/v1alpha1, Resource=lvmclusters", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=prometheuses": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=prometheuses", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=hostfirmwaresettings": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=hostfirmwaresettings", couldn't start monitor for resource "ingress.operator.openshift.io/v1, Resource=dnsrecords": unable to monitor quota for resource "ingress.operator.openshift.io/v1, Resource=dnsrecords", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=podmonitors": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=podmonitors", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=firmwareschemas": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=firmwareschemas", couldn't start monitor for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=overlappingrangeipreservations": unable to monitor quota for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=overlappingrangeipreservations", couldn't start monitor for resource "lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroupnodestatuses": unable to monitor quota for resource "lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroupnodestatuses", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=preprovisioningimages": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=preprovisioningimages", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=baremetalhosts": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=baremetalhosts", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machines": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machines", couldn't start monitor for resource "machine.openshift.io/v1, Resource=controlplanemachinesets": unable to monitor quota for resource "machine.openshift.io/v1, Resource=controlplanemachinesets", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=probes": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=probes", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=clusterserviceversions": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=clusterserviceversions", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=subscriptions": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=subscriptions", couldn't start monitor for resource "k8s.cni.cncf.io/v1, Resource=network-attachment-definitions": unable to monitor quota for resource "k8s.cni.cncf.io/v1, Resource=network-attachment-definitions", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=servicemonitors": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=servicemonitors", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machinehealthchecks": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machinehealthchecks", couldn't start monitor for resource "lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroups": unable to monitor quota for resource "lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroups", couldn't start monitor for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=ippools": unable to monitor quota for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=ippools", couldn't start monitor for resource "controlplane.operator.openshift.io/v1alpha1, Resource=podnetworkconnectivitychecks": unable to monitor quota for resource "controlplane.operator.openshift.io/v1alpha1, Resource=podnetworkconnectivitychecks", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machinesets": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machinesets", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=thanosrulers": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=thanosrulers", couldn't start monitor for resource "monitoring.coreos.com/v1beta1, Resource=alertmanagerconfigs": unable to monitor quota for resource "monitoring.coreos.com/v1beta1, Resource=alertmanagerconfigs", couldn't start monitor for resource "network.operator.openshift.io/v1, Resource=egressrouters": unable to monitor quota for resource "network.operator.openshift.io/v1, Resource=egressrouters", couldn't start monitor for resource "network.operator.openshift.io/v1, Resource=operatorpkis": unable to monitor quota for resource "network.operator.openshift.io/v1, Resource=operatorpkis", couldn't start monitor for resource "k8s.ovn.org/v1, Resource=egressqoses": unable to monitor quota for resource "k8s.ovn.org/v1, Resource=egressqoses", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=installplans": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=installplans", couldn't start monitor for resource "k8s.ovn.org/v1, Resource=egressfirewalls": unable to monitor quota for resource "k8s.ovn.org/v1, Resource=egressfirewalls", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=alertmanagers": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=alertmanagers", couldn't start monitor for resource "snapshot.storage.k8s.io/v1, Resource=volumesnapshots": unable to monitor quota for resource "snapshot.storage.k8s.io/v1, Resource=volumesnapshots", couldn't start monitor for resource "operator.openshift.io/v1, Resource=ingresscontrollers": unable to monitor quota for resource "operator.openshift.io/v1, Resource=ingresscontrollers", couldn't start monitor for resource "cloudcredential.openshift.io/v1, Resource=credentialsrequests": unable to monitor quota for resource "cloudcredential.openshift.io/v1, Resource=credentialsrequests", couldn't start monitor for resource "autoscaling.openshift.io/v1beta1, Resource=machineautoscalers": unable to monitor quota for resource "autoscaling.openshift.io/v1beta1, Resource=machineautoscalers", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=hardwaredata": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=hardwaredata", couldn't start monitor for resource "helm.openshift.io/v1beta1, Resource=projecthelmchartrepositories": unable to monitor quota for resource "helm.openshift.io/v1beta1, Resource=projecthelmchartrepositories", couldn't start monitor for resource "operators.coreos.com/v1, Resource=operatorgroups": unable to monitor quota for resource "operators.coreos.com/v1, Resource=operatorgroups", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=bmceventsubscriptions": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=bmceventsubscriptions", couldn't start monitor for resource "operators.coreos.com/v2, Resource=operatorconditions": unable to monitor quota for resource "operators.coreos.com/v2, Resource=operatorconditions", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=prometheusrules": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=prometheusrules", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=catalogsources": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=catalogsources"]
2023-05-04T16:32:31.144286626Z W0504 16:32:31.144235       1 reflector.go:424] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169: failed to list *v1.ImageStream: the server is currently unable to handle the request (get imagestreams.image.openshift.io)
2023-05-04T16:32:31.144286626Z E0504 16:32:31.144272       1 reflector.go:140] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169: Failed to watch *v1.ImageStream: failed to list *v1.ImageStream: the server is currently unable to handle the request (get imagestreams.image.openshift.io)
2023-05-04T16:32:31.174306764Z I0504 16:32:31.174248       1 base_controller.go:73] Caches are synced for WebhookAuthenticatorCertApprover_csr-approver-controller 
2023-05-04T16:32:31.174306764Z I0504 16:32:31.174266       1 base_controller.go:110] Starting #1 worker of WebhookAuthenticatorCertApprover_csr-approver-controller controller ...
2023-05-04T16:32:31.209253615Z I0504 16:32:31.209201       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "router-monitoring" not found
2023-05-04T16:32:31.209253615Z I0504 16:32:31.209219       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "prometheus-adapter" not found
2023-05-04T16:32:31.209253615Z I0504 16:32:31.209237       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:image-import-controller" not found
2023-05-04T16:32:31.209253615Z I0504 16:32:31.209244       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:ttl-controller" not found
2023-05-04T16:32:31.209805240Z I0504 16:32:31.209789       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "console-extensions-reader" not found
2023-05-04T16:32:31.209805240Z I0504 16:32:31.209798       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "openshift-ovn-kubernetes-controller" not found
2023-05-04T16:32:31.209821741Z I0504 16:32:31.209804       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "prometheus-k8s-scheduler-resources" not found
2023-05-04T16:32:31.209821741Z I0504 16:32:31.209809       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.209821741Z I0504 16:32:31.209814       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:build-strategy-docker" not found
2023-05-04T16:32:31.209821741Z I0504 16:32:31.209819       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:image-builder" not found
2023-05-04T16:32:31.209829345Z I0504 16:32:31.209824       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:horizontal-pod-autoscaler" not found
2023-05-04T16:32:31.209835286Z I0504 16:32:31.209828       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.209840566Z I0504 16:32:31.209833       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.209845956Z I0504 16:32:31.209839       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-samples-operator-proxy-reader" not found
2023-05-04T16:32:31.209935294Z I0504 16:32:31.209845       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:endpoint-controller" not found
2023-05-04T16:32:31.209946555Z I0504 16:32:31.209849       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:kube-dns" not found
2023-05-04T16:32:31.209946555Z I0504 16:32:31.209939       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:oauth-token-deleter" not found
2023-05-04T16:32:31.209946555Z I0504 16:32:31.209943       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.209994956Z I0504 16:32:31.209948       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "vg-manager-clusterrole" not found
2023-05-04T16:32:31.209994956Z I0504 16:32:31.209958       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.209994956Z I0504 16:32:31.209971       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:auth-delegator" not found
2023-05-04T16:32:31.209994956Z I0504 16:32:31.209976       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "openshift-csi-snapshot-controller-runner" not found
2023-05-04T16:32:31.209994956Z I0504 16:32:31.209982       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:default-rolebindings-controller" not found
2023-05-04T16:32:31.209994956Z I0504 16:32:31.209986       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "telemeter-client" not found
2023-05-04T16:32:31.209994956Z I0504 16:32:31.209991       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-samples-operator-imageconfig-reader" not found
2023-05-04T16:32:31.210012359Z I0504 16:32:31.209996       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:node-admin" not found
2023-05-04T16:32:31.210012359Z I0504 16:32:31.210001       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "kube-state-metrics" not found
2023-05-04T16:32:31.210012359Z I0504 16:32:31.210005       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "multus" not found
2023-05-04T16:32:31.210018931Z I0504 16:32:31.210010       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:discovery" not found
2023-05-04T16:32:31.210018931Z I0504 16:32:31.210015       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found
2023-05-04T16:32:31.210024441Z I0504 16:32:31.210020       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "thanos-querier" not found
2023-05-04T16:32:31.210029591Z I0504 16:32:31.210025       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-status" not found
2023-05-04T16:32:31.210034600Z I0504 16:32:31.210030       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:namespace-controller" not found
2023-05-04T16:32:31.210039570Z I0504 16:32:31.210035       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:pv-recycler-controller" not found
2023-05-04T16:32:31.210045751Z I0504 16:32:31.210041       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:resourcequota-controller" not found
2023-05-04T16:32:31.210050691Z I0504 16:32:31.210046       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210056982Z I0504 16:32:31.210052       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "console-operator" not found
2023-05-04T16:32:31.210061882Z I0504 16:32:31.210057       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "self-access-reviewer" not found
2023-05-04T16:32:31.210066781Z I0504 16:32:31.210062       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:deploymentconfig-controller" not found
2023-05-04T16:32:31.210071640Z I0504 16:32:31.210067       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:image-trigger-controller" not found
2023-05-04T16:32:31.210077751Z I0504 16:32:31.210073       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:auth-delegator" not found
2023-05-04T16:32:31.210082681Z I0504 16:32:31.210078       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210087500Z I0504 16:32:31.210083       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "topolvm-controller" not found
2023-05-04T16:32:31.210092299Z I0504 16:32:31.210087       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:auth-delegator" not found
2023-05-04T16:32:31.210097138Z I0504 16:32:31.210092       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "machine-api-operator" not found
2023-05-04T16:32:31.210101927Z I0504 16:32:31.210097       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "openshift-state-metrics" not found
2023-05-04T16:32:31.210106746Z I0504 16:32:31.210103       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:useroauthaccesstoken-manager" not found
2023-05-04T16:32:31.210111555Z I0504 16:32:31.210107       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "insights-operator-gather" not found
2023-05-04T16:32:31.210117576Z I0504 16:32:31.210113       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "multus-admission-controller-webhook" not found
2023-05-04T16:32:31.210122485Z I0504 16:32:31.210117       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "prometheus-k8s" not found
2023-05-04T16:32:31.210127294Z I0504 16:32:31.210122       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "topolvm-node" not found
2023-05-04T16:32:31.210132083Z I0504 16:32:31.210127       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-reader" not found
2023-05-04T16:32:31.210138285Z I0504 16:32:31.210132       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-reader" not found
2023-05-04T16:32:31.210143104Z I0504 16:32:31.210136       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "kube-apiserver" not found
2023-05-04T16:32:31.210147923Z I0504 16:32:31.210143       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "lvms-operator.v4.12.1-65455c75cb" not found
2023-05-04T16:32:31.210152732Z I0504 16:32:31.210147       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "marketplace-operator" not found
2023-05-04T16:32:31.210157531Z I0504 16:32:31.210152       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "openshift-dns-operator" not found
2023-05-04T16:32:31.210162400Z I0504 16:32:31.210157       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:openshift-route-controller-manager" not found
2023-05-04T16:32:31.210167239Z I0504 16:32:31.210163       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210172028Z I0504 16:32:31.210168       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "csi-snapshot-controller-operator-clusterrole" not found
2023-05-04T16:32:31.210178100Z I0504 16:32:31.210173       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:template-instance-finalizer-controller" not found
2023-05-04T16:32:31.210232872Z I0504 16:32:31.210178       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found
2023-05-04T16:32:31.210239335Z I0504 16:32:31.210231       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210239335Z I0504 16:32:31.210236       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-monitoring-operator" not found
2023-05-04T16:32:31.210263540Z I0504 16:32:31.210242       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:pvc-protection-controller" not found
2023-05-04T16:32:31.210263540Z I0504 16:32:31.210249       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210263540Z I0504 16:32:31.210255       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:certificate-controller" not found
2023-05-04T16:32:31.210263540Z I0504 16:32:31.210260       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:deployment-controller" not found
2023-05-04T16:32:31.210272687Z I0504 16:32:31.210265       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210272687Z I0504 16:32:31.210269       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:node-proxier" not found
2023-05-04T16:32:31.210279340Z I0504 16:32:31.210275       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:serviceaccount-controller" not found
2023-05-04T16:32:31.210284229Z I0504 16:32:31.210280       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210289108Z I0504 16:32:31.210285       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:monitoring" not found
2023-05-04T16:32:31.210293907Z I0504 16:32:31.210289       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210299918Z I0504 16:32:31.210295       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "machine-config-server" not found
2023-05-04T16:32:31.210304797Z I0504 16:32:31.210300       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:public-info-viewer" not found
2023-05-04T16:32:31.210309636Z I0504 16:32:31.210306       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:service-account-issuer-discovery" not found
2023-05-04T16:32:31.210314455Z I0504 16:32:31.210311       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:image-puller" not found
2023-05-04T16:32:31.210320517Z I0504 16:32:31.210316       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:cluster-csr-approver-controller" not found
2023-05-04T16:32:31.210325506Z I0504 16:32:31.210321       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:tokenreview-openshift-route-controller-manager" not found
2023-05-04T16:32:31.210330335Z I0504 16:32:31.210326       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "alertmanager-main" not found
2023-05-04T16:32:31.210336377Z I0504 16:32:31.210332       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-image-registry-operator" not found
2023-05-04T16:32:31.210343490Z I0504 16:32:31.210338       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:replicaset-controller" not found
2023-05-04T16:32:31.210377564Z I0504 16:32:31.210343       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:openshift-controller-manager:ingress-to-route-controller" not found
2023-05-04T16:32:31.210377564Z I0504 16:32:31.210348       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:operator:cloud-controller-manager" not found
2023-05-04T16:32:31.210377564Z I0504 16:32:31.210354       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "openshift-ingress-operator" not found
2023-05-04T16:32:31.210377564Z I0504 16:32:31.210359       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210377564Z I0504 16:32:31.210364       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:expand-controller" not found
2023-05-04T16:32:31.210377564Z I0504 16:32:31.210369       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-samples-operator" not found
2023-05-04T16:32:31.210377564Z I0504 16:32:31.210373       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "machine-config-daemon" not found
2023-05-04T16:32:31.210387102Z I0504 16:32:31.210379       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:build-config-change-controller" not found
2023-05-04T16:32:31.210387102Z I0504 16:32:31.210383       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210393614Z I0504 16:32:31.210389       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210398613Z I0504 16:32:31.210394       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:image-pruner" not found
2023-05-04T16:32:31.210404675Z I0504 16:32:31.210400       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:auth-delegator" not found
2023-05-04T16:32:31.210409534Z I0504 16:32:31.210405       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:root-ca-cert-publisher" not found
2023-05-04T16:32:31.210414363Z I0504 16:32:31.210410       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:cronjob-controller" not found
2023-05-04T16:32:31.210420745Z I0504 16:32:31.210415       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:ttl-after-finished-controller" not found
2023-05-04T16:32:31.210425594Z I0504 16:32:31.210421       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:service-ingress-ip-controller" not found
2023-05-04T16:32:31.210430393Z I0504 16:32:31.210426       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cloud-controller-manager" not found
2023-05-04T16:32:31.210435202Z I0504 16:32:31.210431       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "insights-operator" not found
2023-05-04T16:32:31.210440091Z I0504 16:32:31.210436       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-monitoring-view" not found
2023-05-04T16:32:31.210446062Z I0504 16:32:31.210442       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:service-ca" not found
2023-05-04T16:32:31.210450881Z I0504 16:32:31.210447       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:template-service-broker" not found
2023-05-04T16:32:31.210455700Z I0504 16:32:31.210452       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210460499Z I0504 16:32:31.210456       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-node-tuning:tuned" not found
2023-05-04T16:32:31.210465329Z I0504 16:32:31.210461       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "machine-api-controllers" not found
2023-05-04T16:32:31.210470138Z I0504 16:32:31.210466       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:operator-lifecycle-manager" not found
2023-05-04T16:32:31.210476169Z I0504 16:32:31.210471       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "openshift-ovn-kubernetes-node" not found
2023-05-04T16:32:31.210491478Z I0504 16:32:31.210480       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:endpointslice-controller" not found
2023-05-04T16:32:31.210491478Z I0504 16:32:31.210487       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:service-ca-cert-publisher" not found
2023-05-04T16:32:31.210497709Z I0504 16:32:31.210493       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "console" not found
2023-05-04T16:32:31.210503801Z I0504 16:32:31.210499       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "openshift-csi-snapshot-controller-runner" not found
2023-05-04T16:32:31.210508730Z I0504 16:32:31.210504       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:openshift-controller-manager:update-buildconfig-status" not found
2023-05-04T16:32:31.210514861Z I0504 16:32:31.210510       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210519680Z I0504 16:32:31.210515       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:machine-approver" not found
2023-05-04T16:32:31.210524479Z I0504 16:32:31.210521       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:ephemeral-volume-controller" not found
2023-05-04T16:32:31.210530411Z I0504 16:32:31.210526       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:tokenreview-openshift-controller-manager" not found
2023-05-04T16:32:31.210536562Z I0504 16:32:31.210532       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:check-endpoints-crd-reader" not found
2023-05-04T16:32:31.210541441Z I0504 16:32:31.210537       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:openshift-controller-manager:image-trigger-controller" not found
2023-05-04T16:32:31.210547523Z I0504 16:32:31.210542       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:pv-protection-controller" not found
2023-05-04T16:32:31.210552412Z I0504 16:32:31.210547       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:node-admin" not found
2023-05-04T16:32:31.210557251Z I0504 16:32:31.210552       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "registry-monitoring" not found
2023-05-04T16:32:31.210562080Z I0504 16:32:31.210557       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:kube-controller-manager" not found
2023-05-04T16:32:31.210566909Z I0504 16:32:31.210562       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "admin" not found
2023-05-04T16:32:31.210571718Z I0504 16:32:31.210567       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:build-strategy-jenkinspipeline" not found
2023-05-04T16:32:31.210576647Z I0504 16:32:31.210572       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:disruption-controller" not found
2023-05-04T16:32:31.210582739Z I0504 16:32:31.210577       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:endpointslicemirroring-controller" not found
2023-05-04T16:32:31.210587598Z I0504 16:32:31.210584       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:serviceaccount-pull-secrets-controller" not found
2023-05-04T16:32:31.210592467Z I0504 16:32:31.210588       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:scope-impersonation" not found
2023-05-04T16:32:31.210598508Z I0504 16:32:31.210594       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:deployer" not found
2023-05-04T16:32:31.210616913Z I0504 16:32:31.210606       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:template-instance-controller" not found
2023-05-04T16:32:31.210631260Z I0504 16:32:31.210621       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:unidling-controller" not found
2023-05-04T16:32:31.210631260Z I0504 16:32:31.210627       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "topolvm-csi-snapshotter" not found
2023-05-04T16:32:31.210637301Z I0504 16:32:31.210633       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cloud-node-manager" not found
2023-05-04T16:32:31.210643062Z I0504 16:32:31.210637       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-monitoring-view" not found
2023-05-04T16:32:31.210656908Z I0504 16:32:31.210646       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:horizontal-pod-autoscaler" not found
2023-05-04T16:32:31.210656908Z I0504 16:32:31.210653       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:resourcequota-controller" not found
2023-05-04T16:32:31.210670744Z I0504 16:32:31.210661       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-baremetal-operator" not found
2023-05-04T16:32:31.210746236Z I0504 16:32:31.210670       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:node-controller" not found
2023-05-04T16:32:31.210746236Z I0504 16:32:31.210675       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:discovery" not found
2023-05-04T16:32:31.210746236Z I0504 16:32:31.210682       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:scc:restricted-v2" not found
2023-05-04T16:32:31.210746236Z I0504 16:32:31.210687       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "node-exporter" not found
2023-05-04T16:32:31.210746236Z I0504 16:32:31.210691       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:basic-user" not found
2023-05-04T16:32:31.210746236Z I0504 16:32:31.210696       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:generic-garbage-collector" not found
2023-05-04T16:32:31.210746236Z I0504 16:32:31.210701       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "control-plane-machine-set-operator" not found
2023-05-04T16:32:31.210746236Z I0504 16:32:31.210706       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "openshift-ingress-router" not found
2023-05-04T16:32:31.210746236Z I0504 16:32:31.210713       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:route-controller" not found
2023-05-04T16:32:31.210746236Z I0504 16:32:31.210720       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:master" not found
2023-05-04T16:32:31.210746236Z I0504 16:32:31.210724       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "helm-chartrepos-viewer" not found
2023-05-04T16:32:31.210746236Z I0504 16:32:31.210740       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:clusterrole-aggregation-controller" not found
2023-05-04T16:32:31.210759290Z I0504 16:32:31.210745       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:pod-garbage-collector" not found
2023-05-04T16:32:31.210759290Z I0504 16:32:31.210750       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:sdn-reader" not found
2023-05-04T16:32:31.210759290Z I0504 16:32:31.210756       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "self-provisioner" not found
2023-05-04T16:32:31.210777113Z I0504 16:32:31.210763       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:persistent-volume-binder" not found
2023-05-04T16:32:31.210777113Z I0504 16:32:31.210774       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:podsecurity-admission-label-syncer-controller" not found
2023-05-04T16:32:31.210784628Z I0504 16:32:31.210779       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "prometheus-operator" not found
2023-05-04T16:32:31.210803974Z I0504 16:32:31.210785       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:daemon-set-controller" not found
2023-05-04T16:32:31.210803974Z I0504 16:32:31.210801       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:service-controller" not found
2023-05-04T16:32:31.210810206Z I0504 16:32:31.210805       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:statefulset-controller" not found
2023-05-04T16:32:31.210815085Z I0504 16:32:31.210811       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210820004Z I0504 16:32:31.210816       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:node-bootstrapper" not found
2023-05-04T16:32:31.210824833Z I0504 16:32:31.210821       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:node-bootstrapper" not found
2023-05-04T16:32:31.210829662Z I0504 16:32:31.210825       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:node-proxier" not found
2023-05-04T16:32:31.210834481Z I0504 16:32:31.210830       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210839270Z I0504 16:32:31.210835       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:auth-delegator" not found
2023-05-04T16:32:31.210844079Z I0504 16:32:31.210840       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:auth-delegator" not found
2023-05-04T16:32:31.210848968Z I0504 16:32:31.210845       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "openstack-cloud-controller-manager" not found
2023-05-04T16:32:31.210854919Z I0504 16:32:31.210851       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:openshift-controller-manager" not found
2023-05-04T16:32:31.210870509Z I0504 16:32:31.210860       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:certificates.k8s.io:certificatesigningrequests:selfnodeclient" not found
2023-05-04T16:32:31.210876229Z I0504 16:32:31.210871       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:build-strategy-source" not found
2023-05-04T16:32:31.210915533Z I0504 16:32:31.210903       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:namespace-security-allocation-controller" not found
2023-05-04T16:32:31.210923779Z I0504 16:32:31.210914       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "metrics-daemon-role" not found
2023-05-04T16:32:31.210923779Z I0504 16:32:31.210919       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:attachdetach-controller" not found
2023-05-04T16:32:31.210929069Z I0504 16:32:31.210924       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:replication-controller" not found
2023-05-04T16:32:31.210933938Z I0504 16:32:31.210929       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210947263Z I0504 16:32:31.210937       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.210947263Z I0504 16:32:31.210943       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "topolvm-csi-resizer" not found
2023-05-04T16:32:31.210954526Z I0504 16:32:31.210950       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cloud-credential-operator-role" not found
2023-05-04T16:32:31.210968412Z I0504 16:32:31.210955       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "machine-config-controller" not found
2023-05-04T16:32:31.211026511Z I0504 16:32:31.211014       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "network-diagnostics" not found
2023-05-04T16:32:31.211026511Z I0504 16:32:31.211024       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:cluster-quota-reconciliation-controller" not found
2023-05-04T16:32:31.211034927Z I0504 16:32:31.211029       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.211040989Z I0504 16:32:31.211034       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.211046930Z I0504 16:32:31.211043       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:deployer-controller" not found
2023-05-04T16:32:31.211051779Z I0504 16:32:31.211047       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.211056698Z I0504 16:32:31.211052       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "dns-monitoring" not found
2023-05-04T16:32:31.211063491Z I0504 16:32:31.211057       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "whereabouts-cni" not found
2023-05-04T16:32:31.211068470Z I0504 16:32:31.211061       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:auth-delegator" not found
2023-05-04T16:32:31.211073269Z I0504 16:32:31.211067       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:kube-controller-manager:gce-cloud-provider" not found
2023-05-04T16:32:31.211078138Z I0504 16:32:31.211074       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.211082987Z I0504 16:32:31.211079       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:job-controller" not found
2023-05-04T16:32:31.211088989Z I0504 16:32:31.211085       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:check-endpoints-node-reader" not found
2023-05-04T16:32:31.211095030Z I0504 16:32:31.211090       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "admin" not found
2023-05-04T16:32:31.211099949Z I0504 16:32:31.211096       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found
2023-05-04T16:32:31.211104818Z I0504 16:32:31.211101       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:build-controller" not found
2023-05-04T16:32:31.211109627Z I0504 16:32:31.211106       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found
2023-05-04T16:32:31.211114446Z I0504 16:32:31.211110       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "topolvm-csi-provisioner" not found
2023-05-04T16:32:31.211119275Z I0504 16:32:31.211115       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "basic-user" not found
2023-05-04T16:32:31.211124125Z I0504 16:32:31.211120       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-autoscaler" not found
2023-05-04T16:32:31.211128954Z I0504 16:32:31.211125       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-autoscaler-operator" not found
2023-05-04T16:32:31.211133763Z I0504 16:32:31.211129       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:node" not found
2023-05-04T16:32:31.211139834Z I0504 16:32:31.211135       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:origin-namespace-controller" not found
2023-05-04T16:32:31.211144713Z I0504 16:32:31.211141       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-admin" not found
2023-05-04T16:32:31.211195228Z I0504 16:32:31.211146       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:webhook" not found
2023-05-04T16:32:31.211195228Z I0504 16:32:31.211152       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "cluster-node-tuning-operator" not found
2023-05-04T16:32:31.211195228Z I0504 16:32:31.211158       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "machine-api-operator-ext-remediation" not found
2023-05-04T16:32:31.211195228Z I0504 16:32:31.211162       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "openshift-dns" not found
2023-05-04T16:32:31.211195228Z I0504 16:32:31.211167       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:controller:service-account-controller" not found
2023-05-04T16:32:31.211195228Z I0504 16:32:31.211173       1 sccrolecache.go:460] failed to retrieve a role for a rolebinding ref: couldn't retrieve clusterrole from role ref: clusterrole.rbac.authorization.k8s.io "system:openshift:controller:service-serving-cert-controller" not found
2023-05-04T16:32:31.254467901Z I0504 16:32:31.254434       1 base_controller.go:73] Caches are synced for namespace-security-allocation-controller 
2023-05-04T16:32:31.254467901Z I0504 16:32:31.254453       1 base_controller.go:110] Starting #1 worker of namespace-security-allocation-controller controller ...
2023-05-04T16:32:31.254497126Z I0504 16:32:31.254491       1 namespace_scc_allocation_controller.go:111] Repairing SCC UID Allocations
2023-05-04T16:32:31.778378643Z I0504 16:32:31.778333       1 base_controller.go:73] Caches are synced for pod-security-admission-label-synchronization-controller 
2023-05-04T16:32:31.778378643Z I0504 16:32:31.778369       1 base_controller.go:110] Starting #1 worker of pod-security-admission-label-synchronization-controller controller ...
2023-05-04T16:32:32.126458791Z W0504 16:32:32.126426       1 reflector.go:424] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169: failed to list *v1.ImageStream: the server is currently unable to handle the request (get imagestreams.image.openshift.io)
2023-05-04T16:32:32.126517161Z E0504 16:32:32.126508       1 reflector.go:140] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169: Failed to watch *v1.ImageStream: failed to list *v1.ImageStream: the server is currently unable to handle the request (get imagestreams.image.openshift.io)
2023-05-04T16:32:32.266284138Z I0504 16:32:32.266245       1 request.go:601] Waited for 1.148890461s due to client-side throttling, not priority and fairness, request: GET:https://api-int.sno.gw.lo:6443/api/v1/podtemplates?limit=500&resourceVersion=0
2023-05-04T16:32:33.119236065Z I0504 16:32:33.119196       1 namespace_scc_allocation_controller.go:116] Repair complete
2023-05-04T16:32:35.201680568Z W0504 16:32:35.201638       1 reflector.go:424] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169: failed to list *v1.ImageStream: the server is currently unable to handle the request (get imagestreams.image.openshift.io)
2023-05-04T16:32:35.201680568Z E0504 16:32:35.201665       1 reflector.go:140] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169: Failed to watch *v1.ImageStream: failed to list *v1.ImageStream: the server is currently unable to handle the request (get imagestreams.image.openshift.io)
2023-05-04T16:32:41.547675178Z W0504 16:32:41.547635       1 reflector.go:424] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169: failed to list *v1.ImageStream: the server is currently unable to handle the request (get imagestreams.image.openshift.io)
2023-05-04T16:32:41.547737384Z E0504 16:32:41.547728       1 reflector.go:140] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169: Failed to watch *v1.ImageStream: failed to list *v1.ImageStream: the server is currently unable to handle the request (get imagestreams.image.openshift.io)
2023-05-04T16:32:53.215609485Z W0504 16:32:53.215557       1 reflector.go:424] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169: failed to list *v1.ImageStream: the server is currently unable to handle the request (get imagestreams.image.openshift.io)
2023-05-04T16:32:53.215609485Z E0504 16:32:53.215582       1 reflector.go:140] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169: Failed to watch *v1.ImageStream: failed to list *v1.ImageStream: the server is currently unable to handle the request (get imagestreams.image.openshift.io)
2023-05-04T16:33:01.128747170Z E0504 16:33:01.128697       1 reconciliation_controller.go:167] unable to retrieve the complete list of server APIs: apps.openshift.io/v1: the server is currently unable to handle the request, authorization.openshift.io/v1: the server is currently unable to handle the request, build.openshift.io/v1: the server is currently unable to handle the request, image.openshift.io/v1: the server is currently unable to handle the request, metrics.k8s.io/v1beta1: the server is currently unable to handle the request, oauth.openshift.io/v1: the server is currently unable to handle the request, packages.operators.coreos.com/v1: the server is currently unable to handle the request, project.openshift.io/v1: the server is currently unable to handle the request, quota.openshift.io/v1: the server is currently unable to handle the request, route.openshift.io/v1: the server is currently unable to handle the request, security.openshift.io/v1: the server is currently unable to handle the request, template.openshift.io/v1: the server is currently unable to handle the request, user.openshift.io/v1: the server is currently unable to handle the request
2023-05-04T16:33:12.745223639Z W0504 16:33:12.745187       1 reflector.go:424] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169: failed to list *v1.ImageStream: the server is currently unable to handle the request (get imagestreams.image.openshift.io)
2023-05-04T16:33:12.745223639Z E0504 16:33:12.745211       1 reflector.go:140] k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169: Failed to watch *v1.ImageStream: failed to list *v1.ImageStream: the server is currently unable to handle the request (get imagestreams.image.openshift.io)
2023-05-04T16:33:31.189820044Z E0504 16:33:31.183176       1 reconciliation_controller.go:167] unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request
2023-05-04T16:33:31.189820044Z I0504 16:33:31.183225       1 reconciliation_controller.go:203] syncing resource quota controller with updated resources from discovery: map[/v1, Resource=configmaps:{} /v1, Resource=endpoints:{} /v1, Resource=events:{} /v1, Resource=limitranges:{} /v1, Resource=persistentvolumeclaims:{} /v1, Resource=pods:{} /v1, Resource=podtemplates:{} /v1, Resource=replicationcontrollers:{} /v1, Resource=resourcequotas:{} /v1, Resource=secrets:{} /v1, Resource=serviceaccounts:{} /v1, Resource=services:{} apps/v1, Resource=controllerrevisions:{} apps/v1, Resource=daemonsets:{} apps/v1, Resource=deployments:{} apps/v1, Resource=replicasets:{} apps/v1, Resource=statefulsets:{} apps.openshift.io/v1, Resource=deploymentconfigs:{} authorization.openshift.io/v1, Resource=rolebindingrestrictions:{} autoscaling/v2, Resource=horizontalpodautoscalers:{} autoscaling.openshift.io/v1beta1, Resource=machineautoscalers:{} batch/v1, Resource=cronjobs:{} batch/v1, Resource=jobs:{} build.openshift.io/v1, Resource=buildconfigs:{} build.openshift.io/v1, Resource=builds:{} cloudcredential.openshift.io/v1, Resource=credentialsrequests:{} controlplane.operator.openshift.io/v1alpha1, Resource=podnetworkconnectivitychecks:{} coordination.k8s.io/v1, Resource=leases:{} discovery.k8s.io/v1, Resource=endpointslices:{} events.k8s.io/v1, Resource=events:{} helm.openshift.io/v1beta1, Resource=projecthelmchartrepositories:{} image.openshift.io/v1, Resource=imagestreams:{} ingress.operator.openshift.io/v1, Resource=dnsrecords:{} k8s.cni.cncf.io/v1, Resource=network-attachment-definitions:{} k8s.ovn.org/v1, Resource=egressfirewalls:{} k8s.ovn.org/v1, Resource=egressqoses:{} lvm.topolvm.io/v1alpha1, Resource=lvmclusters:{} lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroupnodestatuses:{} lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroups:{} machine.openshift.io/v1, Resource=controlplanemachinesets:{} machine.openshift.io/v1beta1, Resource=machinehealthchecks:{} machine.openshift.io/v1beta1, Resource=machines:{} machine.openshift.io/v1beta1, Resource=machinesets:{} metal3.io/v1alpha1, Resource=baremetalhosts:{} metal3.io/v1alpha1, Resource=bmceventsubscriptions:{} metal3.io/v1alpha1, Resource=firmwareschemas:{} metal3.io/v1alpha1, Resource=hardwaredata:{} metal3.io/v1alpha1, Resource=hostfirmwaresettings:{} metal3.io/v1alpha1, Resource=preprovisioningimages:{} monitoring.coreos.com/v1, Resource=alertmanagers:{} monitoring.coreos.com/v1, Resource=podmonitors:{} monitoring.coreos.com/v1, Resource=probes:{} monitoring.coreos.com/v1, Resource=prometheuses:{} monitoring.coreos.com/v1, Resource=prometheusrules:{} monitoring.coreos.com/v1, Resource=servicemonitors:{} monitoring.coreos.com/v1, Resource=thanosrulers:{} monitoring.coreos.com/v1beta1, Resource=alertmanagerconfigs:{} network.operator.openshift.io/v1, Resource=egressrouters:{} network.operator.openshift.io/v1, Resource=operatorpkis:{} networking.k8s.io/v1, Resource=ingresses:{} networking.k8s.io/v1, Resource=networkpolicies:{} operator.openshift.io/v1, Resource=ingresscontrollers:{} operators.coreos.com/v1, Resource=operatorgroups:{} operators.coreos.com/v1alpha1, Resource=catalogsources:{} operators.coreos.com/v1alpha1, Resource=clusterserviceversions:{} operators.coreos.com/v1alpha1, Resource=installplans:{} operators.coreos.com/v1alpha1, Resource=subscriptions:{} operators.coreos.com/v2, Resource=operatorconditions:{} policy/v1, Resource=poddisruptionbudgets:{} rbac.authorization.k8s.io/v1, Resource=rolebindings:{} rbac.authorization.k8s.io/v1, Resource=roles:{} route.openshift.io/v1, Resource=routes:{} snapshot.storage.k8s.io/v1, Resource=volumesnapshots:{} storage.k8s.io/v1, Resource=csistoragecapacities:{} template.openshift.io/v1, Resource=templateinstances:{} template.openshift.io/v1, Resource=templates:{} tuned.openshift.io/v1, Resource=profiles:{} tuned.openshift.io/v1, Resource=tuneds:{} whereabouts.cni.cncf.io/v1alpha1, Resource=ippools:{} whereabouts.cni.cncf.io/v1alpha1, Resource=overlappingrangeipreservations:{}]
2023-05-04T16:33:31.189820044Z I0504 16:33:31.183773       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for deploymentconfigs.apps.openshift.io
2023-05-04T16:33:31.189820044Z I0504 16:33:31.183805       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for buildconfigs.build.openshift.io
2023-05-04T16:33:31.189820044Z I0504 16:33:31.183836       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for routes.route.openshift.io
2023-05-04T16:33:31.189820044Z I0504 16:33:31.183910       1 resource_quota_monitor.go:218] QuotaMonitor created object count evaluator for builds.build.openshift.io
2023-05-04T16:33:31.189820044Z E0504 16:33:31.184105       1 reconciliation_controller.go:213] failed to sync resource monitors: [couldn't start monitor for resource "k8s.cni.cncf.io/v1, Resource=network-attachment-definitions": unable to monitor quota for resource "k8s.cni.cncf.io/v1, Resource=network-attachment-definitions", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machinesets": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machinesets", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=servicemonitors": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=servicemonitors", couldn't start monitor for resource "operator.openshift.io/v1, Resource=ingresscontrollers": unable to monitor quota for resource "operator.openshift.io/v1, Resource=ingresscontrollers", couldn't start monitor for resource "lvm.topolvm.io/v1alpha1, Resource=lvmclusters": unable to monitor quota for resource "lvm.topolvm.io/v1alpha1, Resource=lvmclusters", couldn't start monitor for resource "network.operator.openshift.io/v1, Resource=egressrouters": unable to monitor quota for resource "network.operator.openshift.io/v1, Resource=egressrouters", couldn't start monitor for resource "tuned.openshift.io/v1, Resource=profiles": unable to monitor quota for resource "tuned.openshift.io/v1, Resource=profiles", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=thanosrulers": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=thanosrulers", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=catalogsources": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=catalogsources", couldn't start monitor for resource "tuned.openshift.io/v1, Resource=tuneds": unable to monitor quota for resource "tuned.openshift.io/v1, Resource=tuneds", couldn't start monitor for resource "ingress.operator.openshift.io/v1, Resource=dnsrecords": unable to monitor quota for resource "ingress.operator.openshift.io/v1, Resource=dnsrecords", couldn't start monitor for resource "network.operator.openshift.io/v1, Resource=operatorpkis": unable to monitor quota for resource "network.operator.openshift.io/v1, Resource=operatorpkis", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=hardwaredata": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=hardwaredata", couldn't start monitor for resource "autoscaling.openshift.io/v1beta1, Resource=machineautoscalers": unable to monitor quota for resource "autoscaling.openshift.io/v1beta1, Resource=machineautoscalers", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=probes": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=probes", couldn't start monitor for resource "operators.coreos.com/v2, Resource=operatorconditions": unable to monitor quota for resource "operators.coreos.com/v2, Resource=operatorconditions", couldn't start monitor for resource "cloudcredential.openshift.io/v1, Resource=credentialsrequests": unable to monitor quota for resource "cloudcredential.openshift.io/v1, Resource=credentialsrequests", couldn't start monitor for resource "authorization.openshift.io/v1, Resource=rolebindingrestrictions": unable to monitor quota for resource "authorization.openshift.io/v1, Resource=rolebindingrestrictions", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=prometheuses": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=prometheuses", couldn't start monitor for resource "snapshot.storage.k8s.io/v1, Resource=volumesnapshots": unable to monitor quota for resource "snapshot.storage.k8s.io/v1, Resource=volumesnapshots", couldn't start monitor for resource "controlplane.operator.openshift.io/v1alpha1, Resource=podnetworkconnectivitychecks": unable to monitor quota for resource "controlplane.operator.openshift.io/v1alpha1, Resource=podnetworkconnectivitychecks", couldn't start monitor for resource "k8s.ovn.org/v1, Resource=egressfirewalls": unable to monitor quota for resource "k8s.ovn.org/v1, Resource=egressfirewalls", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machinehealthchecks": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machinehealthchecks", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=clusterserviceversions": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=clusterserviceversions", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=bmceventsubscriptions": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=bmceventsubscriptions", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=baremetalhosts": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=baremetalhosts", couldn't start monitor for resource "template.openshift.io/v1, Resource=templates": unable to monitor quota for resource "template.openshift.io/v1, Resource=templates", couldn't start monitor for resource "k8s.ovn.org/v1, Resource=egressqoses": unable to monitor quota for resource "k8s.ovn.org/v1, Resource=egressqoses", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=prometheusrules": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=prometheusrules", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=installplans": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=installplans", couldn't start monitor for resource "lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroups": unable to monitor quota for resource "lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroups", couldn't start monitor for resource "machine.openshift.io/v1beta1, Resource=machines": unable to monitor quota for resource "machine.openshift.io/v1beta1, Resource=machines", couldn't start monitor for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=ippools": unable to monitor quota for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=ippools", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=hostfirmwaresettings": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=hostfirmwaresettings", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=alertmanagers": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=alertmanagers", couldn't start monitor for resource "machine.openshift.io/v1, Resource=controlplanemachinesets": unable to monitor quota for resource "machine.openshift.io/v1, Resource=controlplanemachinesets", couldn't start monitor for resource "operators.coreos.com/v1alpha1, Resource=subscriptions": unable to monitor quota for resource "operators.coreos.com/v1alpha1, Resource=subscriptions", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=preprovisioningimages": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=preprovisioningimages", couldn't start monitor for resource "helm.openshift.io/v1beta1, Resource=projecthelmchartrepositories": unable to monitor quota for resource "helm.openshift.io/v1beta1, Resource=projecthelmchartrepositories", couldn't start monitor for resource "operators.coreos.com/v1, Resource=operatorgroups": unable to monitor quota for resource "operators.coreos.com/v1, Resource=operatorgroups", couldn't start monitor for resource "lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroupnodestatuses": unable to monitor quota for resource "lvm.topolvm.io/v1alpha1, Resource=lvmvolumegroupnodestatuses", couldn't start monitor for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=overlappingrangeipreservations": unable to monitor quota for resource "whereabouts.cni.cncf.io/v1alpha1, Resource=overlappingrangeipreservations", couldn't start monitor for resource "template.openshift.io/v1, Resource=templateinstances": unable to monitor quota for resource "template.openshift.io/v1, Resource=templateinstances", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=podmonitors": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=podmonitors", couldn't start monitor for resource "monitoring.coreos.com/v1beta1, Resource=alertmanagerconfigs": unable to monitor quota for resource "monitoring.coreos.com/v1beta1, Resource=alertmanagerconfigs", couldn't start monitor for resource "metal3.io/v1alpha1, Resource=firmwareschemas": unable to monitor quota for resource "metal3.io/v1alpha1, Resource=firmwareschemas"]
2023-05-04T16:33:31.191165787Z I0504 16:33:31.191131       1 resource_quota_controller.go:443] syncing resource quota controller with updated resources from discovery: added: [image.openshift.io/v1, Resource=imagestreams], removed: []
2023-05-04T16:33:31.191222313Z W0504 16:33:31.191206       1 shared_informer.go:533] resyncPeriod 5m47.570384024s is smaller than resyncCheckPeriod 10m0s and the informer has already started. Changing it to 10m0s
2023-05-04T16:33:31.191253461Z I0504 16:33:31.191239       1 shared_informer.go:255] Waiting for caches to sync for resource quota
2023-05-04T16:34:01.191328908Z I0504 16:34:01.191293       1 shared_informer.go:281] stop requested
2023-05-04T16:34:01.191391505Z E0504 16:34:01.191382       1 shared_informer.go:258] unable to sync caches for resource quota
2023-05-04T16:34:01.191411493Z E0504 16:34:01.191405       1 resource_quota_controller.go:456] timed out waiting for quota monitor sync
2023-05-04T16:34:01.910476992Z I0504 16:34:01.910444       1 shared_informer.go:262] Caches are synced for resource quota
2023-05-04T16:34:31.253374019Z I0504 16:34:31.253336       1 resource_quota_controller.go:443] syncing resource quota controller with updated resources from discovery: added: [image.openshift.io/v1, Resource=imagestreams], removed: []
2023-05-04T16:34:31.253374019Z I0504 16:34:31.253357       1 shared_informer.go:255] Waiting for caches to sync for resource quota
2023-05-04T16:34:31.253374019Z I0504 16:34:31.253366       1 shared_informer.go:262] Caches are synced for resource quota
2023-05-04T16:34:31.253374019Z I0504 16:34:31.253369       1 resource_quota_controller.go:462] synced quota controller
2023-05-04T18:46:28.901404642Z I0504 18:46:28.901299       1 event.go:285] Event(v1.ObjectReference{Kind:"Pod", Namespace:"openshift-kube-controller-manager", Name:"kube-controller-manager-node", UID:"", APIVersion:"", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'CreatedSCCRanges' created SCC ranges for openshift-must-gather-7nvg4 namespace
